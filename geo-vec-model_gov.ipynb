{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geo-Vec Model \n",
    "- basic geo-vec model\n",
    "- auxilliary task models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/govert/anaconda3/envs/pytorch-par2vec/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as ss\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load tokenized reuters\n",
    "word2id = np.load('data/reuters/reuters_word2id.npy').item(0)\n",
    "id2word = np.load('data/reuters/reuters_id2word.npy').item(0)\n",
    "tokenized = list(np.load('data/reuters/reuters_tokenized.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.corpora.wikicorpus import WikiCorpus\n",
    "from gensim import utils\n",
    "# from scipy.sparse import coo_matrix\n",
    "\n",
    "def get_adj(tokenized_docs, word2id):\n",
    "    for docidx in tokenized_docs:\n",
    "        adj_i = np.vstack((docidx[:-1], docidx[1:]))\n",
    "        adj_o = np.flip(adj_i, axis=0)\n",
    "        sp_adj_i = ss.coo_matrix((np.ones(adj_i.shape[1]), (adj_i[0, :], adj_i[1, :])), \n",
    "                                 (len(word2id), len(word2id)))\n",
    "        sp_adj_o = ss.coo_matrix((np.ones(adj_o.shape[1]), (adj_o[0, :], adj_o[1, :])), \n",
    "                                 (len(word2id), len(word2id)))\n",
    "        yield sp_adj_o, sp_adj_i\n",
    "\n",
    "def get_lapl(tokenized_docs, word2id, renorm_trick=True):\n",
    "    for A_o, A_i in get_adj(tokenized_docs, word2id):\n",
    "        if renorm_trick == True:\n",
    "            _A_i = A_i + ss.eye(A_i.shape[0])\n",
    "            _A_o = A_o + ss.eye(A_o.shape[0])\n",
    "        D_inv_sqrt_i = ss.diags(np.power(np.array(_A_i.sum(1)), -0.5).flatten())\n",
    "        D_inv_sqrt_o = ss.diags(np.power(np.array(_A_o.sum(1)), -0.5).flatten())\n",
    "        L_i = _A_i.dot(D_inv_sqrt_i).transpose().dot(D_inv_sqrt_i).tocoo()\n",
    "        L_o = _A_o.dot(D_inv_sqrt_o).transpose().dot(D_inv_sqrt_o).tocoo()\n",
    "        \n",
    "        yield A_o, A_i, L_o, L_i\n",
    "        \n",
    "Ls = []\n",
    "for A_o, A_i, L_o, L_i in get_lapl(tokenized[:10], word2id):\n",
    "     Ls.append([A_o, A_i, L_o, L_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class WikiCorpusExtended(WikiCorpus):\n",
    "    \"\"\"\n",
    "        Extension on the WikiCorpus from gensim\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, args):\n",
    "        super().__init__(args)\n",
    "        \n",
    "    def get_docidx(self):\n",
    "        for doc in self.get_texts():\n",
    "            doc = [word if isinstance(word, str) else str(word, 'utf-8') for word in doc]\n",
    "            yield np.array([wiki.dictionary.token2id.get(word) for word in doc])\n",
    "    \n",
    "    def get_adj(self):\n",
    "        for docidx in self.get_docidx():\n",
    "            adj_i = np.vstack((docidx[:-1], docidx[1:]))\n",
    "            adj_o = np.flip(np.vstack((docidx[:-1], docidx[1:])), axis=0)\n",
    "            sp_adj_i = ss.coo_matrix((np.ones(adj_i.shape[1]), (adj_i[0, :], adj_i[1, :])), \n",
    "                                     (len(self.dictionary), len(self.dictionary)))\n",
    "            sp_adj_o = ss.coo_matrix((np.ones(adj_o.shape[1]), (adj_o[0, :], adj_o[1, :])), \n",
    "                                     (len(self.dictionary), len(self.dictionary)))\n",
    "            yield sp_adj_i, sp_adj_o\n",
    "            \n",
    "    def get_lapl(self, renorm_trick=True):\n",
    "        for A_i, A_o in self.get_adj():\n",
    "            if renorm_trick == True:\n",
    "                A_i += ss.eye(A_i.shape[0])\n",
    "                A_o += ss.eye(A_o.shape[0])\n",
    "            D_inv_sqrt_i = ss.diags(np.power(np.array(A_i.sum(1)), -0.5).flatten())\n",
    "            D_inv_sqrt_o = ss.diags(np.power(np.array(A_o.sum(1)), -0.5).flatten())\n",
    "            L_i = A_i.dot(D_inv_sqrt_i).transpose().dot(D_inv_sqrt_i).tocoo()\n",
    "            L_o = A_o.dot(D_inv_sqrt_o).transpose().dot(D_inv_sqrt_o).tocoo()\n",
    "            yield L_i, L_o\n",
    "            \n",
    "            \n",
    "# wiki = WikiCorpusExtended('process/enwiki-latest-pages-articles1.xml-p10p30302.bz2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Doc2Graph():\n",
    "    \"\"\"Convert tokenized document to weighted Adjacency matrix\n",
    "    and graph Laplacian\"\"\"\n",
    "    def __init__(self, doc, doc_id=-1):\n",
    "        self.doc = doc\n",
    "        self.doc_id = doc_id\n",
    "        \n",
    "    def doc2graph(self):\n",
    "        g = self.load()\n",
    "            \n",
    "        if not g:\n",
    "            As = self.get_As()\n",
    "            Ls = self.get_Ls(As)\n",
    "            g = As + Ls\n",
    "            self.save(g)\n",
    "\n",
    "        return g\n",
    "    \n",
    "    def get_As(self):\n",
    "        \"\"\"Get the weighted adjacency matrices of incoming\n",
    "        and outcoming edges\"\"\"\n",
    "        As = []\n",
    "        e1 = np.vstack((self.doc[:-1], self.doc[1:])).T\n",
    "        e2 = np.flip(e1, 1)\n",
    "        for a in [e2, e1]:\n",
    "            rc, cooc = np.unique(a, return_counts=True, axis=0)\n",
    "            As.append(ss.coo_matrix((cooc, (rc[:,0], rc[:,1])), \n",
    "                                   tuple((np.max(a)+1, np.max(a)+1))))\n",
    "        return As\n",
    "    \n",
    "    def get_Ls(self, As, renorm_trick=False):\n",
    "        \"\"\"Create graph Laplacians from adjacency matrices\"\"\"\n",
    "        Ls = []\n",
    "        for A in As:\n",
    "            A = ss.coo_matrix(A)\n",
    "            if renorm_trick:\n",
    "                A_ = A + ss.eye(A.shape[0])\n",
    "            D_inv_sqrt = ss.diags(np.power(np.array(A.sum(1)), -0.5).flatten())\n",
    "            L = A.dot(D_inv_sqrt).transpose().dot(D_inv_sqrt).tocoo()\n",
    "            Ls.append(L)\n",
    "            \n",
    "        return Ls    \n",
    "    \n",
    "    def save(self, g):\n",
    "        \"\"\"Save graph to folder for reuse\"\"\"\n",
    "        print('save: implement me!')\n",
    "        pass\n",
    "    \n",
    "    def load(self):\n",
    "        print('load: implement me!')\n",
    "        return None\n",
    "            \n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    if not ss.isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape\n",
    "\n",
    "def sp2tf(sp_t, shape=None):\n",
    "    t = sparse_to_tuple(sp_t)\n",
    "#     tensor = tf.SparseTensor(t[0],t[1].astype(np.float32),t[2])\n",
    "    if shape is not None:\n",
    "        t[2] == shape\n",
    "    tensor = tf.SparseTensorValue(t[0],t[1].astype(np.float32),t[2])\n",
    "    return tensor\n",
    "\n",
    "def dropout_sparse(x, keep_prob, num_nonzero_elems):\n",
    "    \"\"\"Dropout for sparse tensors. Currently fails for very large sparse tensors (>1M elements)\n",
    "    \"\"\"\n",
    "    noise_shape = [num_nonzero_elems]\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += tf.random_uniform(noise_shape)\n",
    "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "    pre_out = tf.sparse_retain(x, dropout_mask)\n",
    "    return pre_out * (1./keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geo-Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "class GeoVec():\n",
    "    def __init__(self, corpus=None, vocab_size=10, h_layers = [8, 4], \n",
    "                 act = tf.nn.relu, dropout=0.0, learning_rate = 1e-3):\n",
    "        \"\"\"Geo-Vec model as described in the report model section.\"\"\"\n",
    "        \n",
    "        self.corpus = corpus\n",
    "        self.vocab_size = vocab_size\n",
    "        self.h_layers = h_layers\n",
    "        self.act = act\n",
    "        self.dropout = dropout\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # use for plotting\n",
    "        self._loss_vals, self._acc_vals = [], []\n",
    "        \n",
    "        #placeholders\n",
    "        s = [self.vocab_size, self.vocab_size]\n",
    "        self.placeholders = {\n",
    "            'A_o': tf.sparse_placeholder(tf.float32),\n",
    "            'L_o': tf.sparse_placeholder(tf.float32),\n",
    "            'A_i': tf.sparse_placeholder(tf.float32),\n",
    "            'L_i': tf.sparse_placeholder(tf.float32),\n",
    "            'idx_i': tf.placeholder(tf.int64),\n",
    "            'idx_o': tf.placeholder(tf.int64),\n",
    "            'val_i': tf.placeholder(tf.float32),\n",
    "            'val_o': tf.placeholder(tf.float32),\n",
    "            'dropout': tf.placeholder_with_default(0., shape=())\n",
    "        }\n",
    "        \n",
    "        # model\n",
    "        self.aux_losses = None\n",
    "        dummy = sp2tf(ss.eye(self.vocab_size))\n",
    "        self.init_model(x=dummy)\n",
    "\n",
    "        #optimizer\n",
    "        self.init_optimizer()\n",
    "        \n",
    "        #sess\n",
    "        self.trained = 0\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def init_model(self, x, aux_tasks = None):\n",
    "        \"\"\"geo-vec model with variable number of gcn layers. Optional aux_taks\n",
    "        param is now unimplemented to specify which tasks to add. All aux losses\n",
    "        should be gathered in a self.aux_losses variable to gather later on.\"\"\"\n",
    "        for i, h_layer in enumerate(self.h_layers):\n",
    "            if i == 0:\n",
    "                h = self.gcn(x, self.vocab_size, self.h_layers[0], self.act, layer=i,sparse=True)  \n",
    "            elif (i+1) < len(self.h_layers):\n",
    "                h = self.gcn(h, self.h_layers[i-1], h_layer, self.act, layer=i, )\n",
    "            else:\n",
    "                self.emb_o, self.emb_i = self.gcn(h, self.h_layers[i-1], \n",
    "                                             h_layer, act=lambda x: x, layer=i,separate=True)\n",
    "                \n",
    "        # here we can left multiply the last layer h\n",
    "        # and perform auxilliary tasks.\n",
    "        posneg_samples_o = tf.gather(self.emb_o, tf.transpose(self.placeholders['idx_o']))\n",
    "        posneg_samples_i = tf.gather(self.emb_i, tf.transpose(self.placeholders['idx_i']))\n",
    "        \n",
    "        self.recon_o = self.decode(posneg_samples_o)\n",
    "        self.recon_i = self.decode(posneg_samples_i)\n",
    "    \n",
    "    def gcn(self, x, dim_in, dim_out, act, layer, sparse=False, separate=False):\n",
    "        \"\"\"basic graph convolution using a split up adjacency matrix.\n",
    "        The separation param is to create the final embeddings to reconstruct.\"\"\"\n",
    "        w1 = tf.get_variable('w1_{}'.format(layer), shape=[dim_in, dim_out], \n",
    "                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "        w2 = tf.get_variable('w2_{}'.format(layer), shape=[dim_in, dim_out], \n",
    "                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        if sparse:\n",
    "            x1 = tf.sparse_tensor_dense_matmul(x, w1)\n",
    "            x2 = tf.sparse_tensor_dense_matmul(x, w2)\n",
    "        else:\n",
    "            x1 = tf.matmul(x, w1)\n",
    "            x2 = tf.matmul(x, w2)\n",
    "            \n",
    "        x1 = tf.sparse_tensor_dense_matmul(self.placeholders['L_o'], x1)\n",
    "        x2 = tf.sparse_tensor_dense_matmul(self.placeholders['L_i'], x2)\n",
    "        \n",
    "        if separate:\n",
    "            return self.act(x1), self.act(x2)\n",
    "        \n",
    "        return self.act(x1 + x2)\n",
    "    \n",
    "    def decode(self, x, cap = 1000):\n",
    "        \"\"\"simple innerproduct decoder with sigmoid activation to scale\n",
    "        the edged between 0-1000 (assuming more co-occurances are unlikely).\"\"\"\n",
    "#         print(x)\n",
    "#         print(x.shape)\n",
    "#         a_t = x\n",
    "#         idx = tf.where(tf.not_equal(a_t, 0))\n",
    "#         # Use tf.shape(a_t, out_type=tf.int64) instead of a_t.get_shape() if tensor shape is dynamic\n",
    "#         x = tf.SparseTensor(idx, tf.gather_nd(a_t, idx), a_t.get_shape())\n",
    "        \n",
    "        x = tf.nn.dropout(x, 1-self.dropout)\n",
    "        \n",
    "#         zero = tf.constant(0, dtype=tf.float32)\n",
    "#         A_rows = tf.sparse_reduce_sum(tf.sparse_add(self.placeholders['A_o'], sp2tf(-ss.eye(self.vocab_size))), 0)\n",
    "\n",
    "#         where = tf.not_equal(A_rows, zero)\n",
    "#         indices = tf.where(where)\n",
    "#         x = tf.gather_nd(x, tf.transpose(indices))\n",
    "        \n",
    "        x = tf.reshape(tf.matmul(x, tf.transpose(x)), [-1])\n",
    "        \n",
    "\n",
    "        return tf.nn.relu(x)\n",
    "        \n",
    "    def init_optimizer(self):\n",
    "        \"\"\"initializes optimizer and computes loss + accuracy. The loss function\n",
    "        is currently a MSE, due to the fact we are dealing with weighted edges.\n",
    "        This does not seem ideal, and should be thought about.\"\"\"\n",
    "        labels_o = self.recon_o\n",
    "        labels_i = self.recon_i\n",
    "#         labels_o = tf.reshape(tf.sparse_tensor_to_dense(\n",
    "#                                 tf.gather(self.placeholders['A_i'], tf.transpose(self.placeholders['idx_i'])),\n",
    "#                                 validate_indices=False), [-1])\n",
    "#         labels_i = tf.reshape(tf.sparse_tensor_to_dense(\n",
    "#                                 tf.gather(self.placeholders['A_i'], tf.transpose(self.placeholders['idx_i'])),\n",
    "#                                 validate_indices=False), [-1])\n",
    "        \n",
    "        emb_or = tf.gather(self.emb_o, self.placeholders['idx_o'][:, 0])\n",
    "        emb_oc = tf.gather(self.emb_o, self.placeholders['idx_o'][:, 1])\n",
    "    \n",
    "        emb_ir = tf.gather(self.emb_i, self.placeholders['idx_i'][:, 0])\n",
    "        emb_ic = tf.gather(self.emb_i, self.placeholders['idx_i'][:, 1])\n",
    "        \n",
    "        self.recon_o = tf.reduce_sum(tf.multiply(emb_or, emb_oc), 1)\n",
    "        self.recon_i = tf.reduce_sum(tf.multiply(emb_ir, emb_ic), 1)\n",
    "        \n",
    "        loss_o = tf.losses.mean_squared_error(self.recon_o, self.placeholders['val_o'])\n",
    "        loss_i = tf.losses.mean_squared_error(self.recon_i, self.placeholders['val_i']) \n",
    "        self.loss = loss_o + loss_i\n",
    "        \n",
    "        # gather aux losses and add to total loss\n",
    "        if self.aux_losses:\n",
    "            self.loss += self.aux_losses\n",
    "        \n",
    "        # optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        self.opt_op = optimizer.minimize(self.loss)\n",
    "\n",
    "        cp_o = tf.equal(tf.cast(self.recon_o, tf.int32), tf.cast(self.placeholders['val_o'], tf.int32))\n",
    "        cp_i = tf.equal(tf.cast(self.recon_i, tf.int32), tf.cast(self.placeholders['val_i'], tf.int32))\n",
    "        correct_prediction = tf.concat([cp_o, cp_i], 0)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    def get_feed_dict(self, A_o, A_i, L_o, L_i, idx_i, idx_o, val_o, val_i):\n",
    "        feed_dict = {self.placeholders['A_o']: A_o,\n",
    "                     self.placeholders['A_i']: A_i,\n",
    "                     self.placeholders['L_o']: L_o,\n",
    "                     self.placeholders['L_i']: L_i,\n",
    "                     self.placeholders['idx_o']: idx_o,\n",
    "                     self.placeholders['idx_i']: idx_i,\n",
    "                     self.placeholders['val_o']: val_o,\n",
    "                     self.placeholders['val_i']: val_i}\n",
    "        return feed_dict\n",
    "    \n",
    "    def get_sample(self, batch_size=64, ratio=1.0):\n",
    "        \"\"\"get random sample from corpus graph cache\"\"\"\n",
    "        dummy = random.choice(Ls).copy()\n",
    "        \n",
    "        pos_idx_o = np.random.choice(range(len(dummy[0].row)), batch_size)\n",
    "        pos_idx_i = np.random.choice(range(len(dummy[1].row)), batch_size)\n",
    "        \n",
    "        idx_o = np.array(list(zip(dummy[0].row, dummy[0].col)))[pos_idx_o, :]\n",
    "        idx_i = np.array(list(zip(dummy[1].row, dummy[1].col)))[pos_idx_i, :]\n",
    "        val_o = dummy[0].data[pos_idx_o]\n",
    "        val_i = dummy[1].data[pos_idx_i]\n",
    "        \n",
    "        for i, d in enumerate(dummy):\n",
    "            dummy[i] = sp2tf(d)\n",
    "\n",
    "        return dummy, idx_o, idx_i, val_o, val_i\n",
    "    \n",
    "    def train(self, num_epochs = 100, print_freq=50):\n",
    "        \"\"\"train op that can be invoked multiple times.\"\"\"\n",
    "        tf.set_random_seed(42)\n",
    "        np.random.seed(42)\n",
    "\n",
    "        for e in range(num_epochs):\n",
    "            self.trained += 1\n",
    "            (A_o, A_i, L_o, L_i), idx_o, idx_i, val_o, val_i = self.get_sample()\n",
    "            \n",
    "            feed_dict = self.get_feed_dict(A_o, A_i, L_o, L_i, idx_o, idx_i, val_o, val_i)\n",
    "            \n",
    "#             idx = np.random.choice(self.placeholders['A_o'].indices[:,0], size=(10,1))\n",
    "#             idx = tf.multinomial(self.placeholders['A_o'].indices[:,0])\n",
    "#             pos_idx = np.random.choice(idx)\n",
    "#             x = tf.sparse_slice(self.placeholders['A_o'], self.placeholders['A_o'].indices[:,0], tf.ones(self.placeholders['A_o'].indices[:,0].shape[1]))\n",
    "#             x = tf.gather(self.emb_o, idx_o)\n",
    "#             o = self.sess.run([x], feed_dict=feed_dict)\n",
    "            \n",
    "            outs = self.sess.run([self.opt_op, self.loss, self.accuracy], feed_dict=feed_dict)\n",
    "            avg_loss, avg_acc = outs[1], outs[2]\n",
    "            self._loss_vals.append(avg_loss)\n",
    "            self._acc_vals.append(avg_acc)\n",
    "            \n",
    "            print('\\r epoch: %d/%d \\t loss: %.3f \\t avg_acc: %.3f' \n",
    "                      % (e+1, num_epochs, avg_loss, avg_acc), end='')\n",
    "            if (e + 1) % print_freq == 0:\n",
    "                print('')\n",
    "        else:\n",
    "            print('----> done training: {} epochs'.format(self.trained))\n",
    "        \n",
    "    def plot(self):\n",
    "        \"\"\"Plotting loss function\"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(self._loss_vals, color='red')\n",
    "        plt.plot(self._acc_vals, color='blue')\n",
    "        \n",
    "        plt.legend(handles=[mpatches.Patch(color='red', label='loss'),\n",
    "                            mpatches.Patch(color='blue', label='acc')],\n",
    "                   bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "        plt.show()\n",
    "        \n",
    "    def get_reconstruction(self, doc = None):\n",
    "        if doc:\n",
    "            A_o, A_i, L_o, L_i = Doc2Graph(doc, doc_id).doc2graph()\n",
    "        else:\n",
    "            (A_o, A_i, L_o, L_i), idx_o, idx_i, val_o, val_i = self.get_sample()\n",
    "#             A_o, A_i, L_o, L_i = self.get_sample()\n",
    "            \n",
    "        feed_dict = self.get_feed_dict(A_o, A_i, L_o, L_i, idx_o, idx_i, val_o, val_i)\n",
    "#         feed_dict = self.get_feed_dict(A_o, A_i, L_o, L_i)\n",
    "        recon_o, recon_i = self.sess.run([self.recon_o, self.recon_i], feed_dict=feed_dict)\n",
    "        return A_o, A_i, recon_o, recon_i\n",
    "    \n",
    "    def get_embeddings(self, doc = None, doc_id = None):\n",
    "        if doc:\n",
    "            A_o, A_i, L_o, L_i = Doc2Graph(doc, doc_id).doc2graph()\n",
    "        else:\n",
    "            (A_o, A_i, L_o, L_i), idx_o, idx_i, val_o, val_i = self.get_sample()\n",
    "#             A_o, A_i, L_o, L_i = self.get_sample()\n",
    "            \n",
    "        feed_dict = self.get_feed_dict(A_o, A_i, L_o, L_i, idx_o, idx_i, val_o, val_i)\n",
    "        \n",
    "#         feed_dict = self.get_feed_dict(A_o, A_i, L_o, L_i, )\n",
    "        emb_o, emb_i = self.sess.run([self.emb_o, self.emb_i], feed_dict=feed_dict)\n",
    "        return A_o, A_i, emb_o, emb_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/govert/anaconda3/envs/pytorch-par2vec/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch: 10/100 \t loss: 1.999 \t avg_acc: 0.000\n",
      " epoch: 20/100 \t loss: 1.998 \t avg_acc: 0.000\n",
      " epoch: 30/100 \t loss: 1.989 \t avg_acc: 0.000\n",
      " epoch: 40/100 \t loss: 1.986 \t avg_acc: 0.000\n",
      " epoch: 50/100 \t loss: 1.969 \t avg_acc: 0.000\n",
      " epoch: 60/100 \t loss: 1.910 \t avg_acc: 0.000\n",
      " epoch: 70/100 \t loss: 1.918 \t avg_acc: 0.000\n",
      " epoch: 80/100 \t loss: 1.789 \t avg_acc: 0.000\n",
      " epoch: 90/100 \t loss: 1.561 \t avg_acc: 0.000\n",
      " epoch: 100/100 \t loss: 1.591 \t avg_acc: 0.000\n",
      "----> done training: 100 epochs\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# fake_doc = np.asarray([1, 2, 3, 4, 1, 5, 6, 4, 1, 7, 9])\n",
    "geo_vec_model = GeoVec(vocab_size=len(word2id), h_layers = [6, 4])\n",
    "geo_vec_model.train(100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "geo_vec_model.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59575"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.0'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(SparseTensorValue(indices=array([[33169,  6109],\n",
       "       [30987, 33169],\n",
       "       [ 6415, 30987],\n",
       "       [ 6088,  6415],\n",
       "       [29294,  6088],\n",
       "       [ 6028, 29294],\n",
       "       [22762,  6028],\n",
       "       [37201, 22762],\n",
       "       [12533, 37201],\n",
       "       [ 7861, 12533],\n",
       "       [34136,  7861],\n",
       "       [32907, 34136],\n",
       "       [16842, 32907],\n",
       "       [54644, 16842],\n",
       "       [48010, 54644],\n",
       "       [46525, 48010],\n",
       "       [41315, 46525],\n",
       "       [57342, 41315],\n",
       "       [54662, 57342],\n",
       "       [50502, 54662],\n",
       "       [55186, 50502],\n",
       "       [50502, 55186],\n",
       "       [48019, 50502],\n",
       "       [41194, 48019],\n",
       "       [56296, 41194],\n",
       "       [38428, 56296],\n",
       "       [59071, 38428],\n",
       "       [57552, 59071],\n",
       "       [37790, 57552],\n",
       "       [38397, 37790],\n",
       "       [37880, 38397],\n",
       "       [50600, 37880],\n",
       "       [49698, 50600],\n",
       "       [55186, 49698],\n",
       "       [57552, 55186],\n",
       "       [31434, 57552],\n",
       "       [22762, 31434],\n",
       "       [35308, 22762],\n",
       "       [50502, 35308],\n",
       "       [20057, 50502],\n",
       "       [32850, 20057],\n",
       "       [45539, 32850],\n",
       "       [43559, 45539],\n",
       "       [34211, 43559],\n",
       "       [41235, 34211],\n",
       "       [54644, 41235],\n",
       "       [57342, 54644],\n",
       "       [59071, 57342],\n",
       "       [38716, 59071],\n",
       "       [44710, 38716],\n",
       "       [45539, 44710],\n",
       "       [45354, 45539],\n",
       "       [59525, 45354],\n",
       "       [38921, 59525],\n",
       "       [37563, 38921],\n",
       "       [52728, 37563],\n",
       "       [52304, 52728],\n",
       "       [50502, 52304],\n",
       "       [43559, 50502],\n",
       "       [51414, 43559],\n",
       "       [55170, 51414],\n",
       "       [ 7861, 55170],\n",
       "       [34136,  7861],\n",
       "       [54644, 34136],\n",
       "       [31434, 54644],\n",
       "       [38294, 31434],\n",
       "       [46525, 38294],\n",
       "       [57342, 46525],\n",
       "       [54395, 57342],\n",
       "       [57552, 54395],\n",
       "       [40147, 57552],\n",
       "       [37880, 40147],\n",
       "       [55186, 37880],\n",
       "       [38428, 55186],\n",
       "       [47276, 38428],\n",
       "       [48019, 47276],\n",
       "       [57643, 48019],\n",
       "       [46791, 57643],\n",
       "       [58573, 46791],\n",
       "       [57552, 58573],\n",
       "       [51345, 57552],\n",
       "       [50502, 51345],\n",
       "       [57342, 50502],\n",
       "       [ 7861, 57342],\n",
       "       [34136,  7861],\n",
       "       [54606, 34136],\n",
       "       [50888, 54606],\n",
       "       [41194, 50888],\n",
       "       [56296, 41194],\n",
       "       [58164, 56296],\n",
       "       [40564, 58164],\n",
       "       [40796, 40564],\n",
       "       [47952, 40796],\n",
       "       [40617, 47952],\n",
       "       [50502, 40617],\n",
       "       [41806, 50502],\n",
       "       [38921, 41806],\n",
       "       [57342, 38921],\n",
       "       [41235, 57342],\n",
       "       [34211, 41235],\n",
       "       [41235, 34211],\n",
       "       [54644, 41235],\n",
       "       [47014, 54644],\n",
       "       [57342, 47014],\n",
       "       [41454, 57342],\n",
       "       [50476, 41454],\n",
       "       [57342, 50476],\n",
       "       [59071, 57342],\n",
       "       [59461, 59071],\n",
       "       [39370, 59461],\n",
       "       [44710, 39370],\n",
       "       [38921, 44710],\n",
       "       [37563, 38921],\n",
       "       [52304, 37563],\n",
       "       [44411, 52304],\n",
       "       [57552, 44411],\n",
       "       [51345, 57552],\n",
       "       [50502, 51345],\n",
       "       [48019, 50502],\n",
       "       [41194, 48019],\n",
       "       [56296, 41194],\n",
       "       [54606, 56296],\n",
       "       [49146, 54606],\n",
       "       [52304, 49146],\n",
       "       [38921, 52304],\n",
       "       [57342, 38921],\n",
       "       [57512, 57342],\n",
       "       [50316, 57512],\n",
       "       [57552, 50316],\n",
       "       [44641, 57552],\n",
       "       [43559, 44641],\n",
       "       [51414, 43559],\n",
       "       [55170, 51414],\n",
       "       [ 7861, 55170],\n",
       "       [34136,  7861],\n",
       "       [38294, 34136],\n",
       "       [54644, 38294],\n",
       "       [48010, 54644],\n",
       "       [55720, 48010],\n",
       "       [57342, 55720],\n",
       "       [57179, 57342],\n",
       "       [54400, 57179],\n",
       "       [57552, 54400],\n",
       "       [48019, 57552],\n",
       "       [10050, 48019],\n",
       "       [21716, 10050],\n",
       "       [47091, 21716],\n",
       "       [57183, 47091],\n",
       "       [47243, 57183],\n",
       "       [38549, 47243],\n",
       "       [45908, 38549],\n",
       "       [47177, 45908],\n",
       "       [57552, 47177],\n",
       "       [37042, 57552],\n",
       "       [16842, 37042],\n",
       "       [50502, 16842],\n",
       "       [15921, 50502],\n",
       "       [34171, 15921],\n",
       "       [45539, 34171],\n",
       "       [43559, 45539],\n",
       "       [ 4997, 43559],\n",
       "       [48010,  4997],\n",
       "       [54644, 48010],\n",
       "       [48010, 54644],\n",
       "       [59461, 48010],\n",
       "       [41751, 59461],\n",
       "       [57552, 41751],\n",
       "       [39370, 57552],\n",
       "       [57342, 39370],\n",
       "       [44682, 57342],\n",
       "       [59444, 44682],\n",
       "       [48625, 59444],\n",
       "       [50502, 48625],\n",
       "       [57342, 50502],\n",
       "       [57183, 57342],\n",
       "       [45539, 57183],\n",
       "       [37042, 45539],\n",
       "       [34211, 37042],\n",
       "       [41235, 34211],\n",
       "       [54644, 41235],\n",
       "       [57342, 54644],\n",
       "       [49901, 57342],\n",
       "       [59194, 49901],\n",
       "       [51212, 59194],\n",
       "       [50502, 51212],\n",
       "       [48019, 50502],\n",
       "       [53837, 48019],\n",
       "       [51714, 53837],\n",
       "       [38428, 51714],\n",
       "       [59461, 38428],\n",
       "       [46660, 59461],\n",
       "       [51314, 46660],\n",
       "       [42283, 51314],\n",
       "       [50637, 42283],\n",
       "       [41992, 50637],\n",
       "       [38428, 41992],\n",
       "       [44322, 38428],\n",
       "       [52419, 44322],\n",
       "       [42698, 52419],\n",
       "       [ 7861, 42698],\n",
       "       [34136,  7861],\n",
       "       [49017, 34136],\n",
       "       [41375, 49017],\n",
       "       [46012, 41375],\n",
       "       [48308, 46012],\n",
       "       [45645, 48308],\n",
       "       [57044, 45645],\n",
       "       [38428, 57044],\n",
       "       [57476, 38428],\n",
       "       [52335, 57476],\n",
       "       [38428, 52335],\n",
       "       [57287, 38428],\n",
       "       [28828, 57287]], dtype=int32), values=array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.], dtype=float32), dense_shape=(59575, 59575)),\n",
       " SparseTensorValue(indices=array([[ 6109, 33169],\n",
       "       [33169, 30987],\n",
       "       [30987,  6415],\n",
       "       [ 6415,  6088],\n",
       "       [ 6088, 29294],\n",
       "       [29294,  6028],\n",
       "       [ 6028, 22762],\n",
       "       [22762, 37201],\n",
       "       [37201, 12533],\n",
       "       [12533,  7861],\n",
       "       [ 7861, 34136],\n",
       "       [34136, 32907],\n",
       "       [32907, 16842],\n",
       "       [16842, 54644],\n",
       "       [54644, 48010],\n",
       "       [48010, 46525],\n",
       "       [46525, 41315],\n",
       "       [41315, 57342],\n",
       "       [57342, 54662],\n",
       "       [54662, 50502],\n",
       "       [50502, 55186],\n",
       "       [55186, 50502],\n",
       "       [50502, 48019],\n",
       "       [48019, 41194],\n",
       "       [41194, 56296],\n",
       "       [56296, 38428],\n",
       "       [38428, 59071],\n",
       "       [59071, 57552],\n",
       "       [57552, 37790],\n",
       "       [37790, 38397],\n",
       "       [38397, 37880],\n",
       "       [37880, 50600],\n",
       "       [50600, 49698],\n",
       "       [49698, 55186],\n",
       "       [55186, 57552],\n",
       "       [57552, 31434],\n",
       "       [31434, 22762],\n",
       "       [22762, 35308],\n",
       "       [35308, 50502],\n",
       "       [50502, 20057],\n",
       "       [20057, 32850],\n",
       "       [32850, 45539],\n",
       "       [45539, 43559],\n",
       "       [43559, 34211],\n",
       "       [34211, 41235],\n",
       "       [41235, 54644],\n",
       "       [54644, 57342],\n",
       "       [57342, 59071],\n",
       "       [59071, 38716],\n",
       "       [38716, 44710],\n",
       "       [44710, 45539],\n",
       "       [45539, 45354],\n",
       "       [45354, 59525],\n",
       "       [59525, 38921],\n",
       "       [38921, 37563],\n",
       "       [37563, 52728],\n",
       "       [52728, 52304],\n",
       "       [52304, 50502],\n",
       "       [50502, 43559],\n",
       "       [43559, 51414],\n",
       "       [51414, 55170],\n",
       "       [55170,  7861],\n",
       "       [ 7861, 34136],\n",
       "       [34136, 54644],\n",
       "       [54644, 31434],\n",
       "       [31434, 38294],\n",
       "       [38294, 46525],\n",
       "       [46525, 57342],\n",
       "       [57342, 54395],\n",
       "       [54395, 57552],\n",
       "       [57552, 40147],\n",
       "       [40147, 37880],\n",
       "       [37880, 55186],\n",
       "       [55186, 38428],\n",
       "       [38428, 47276],\n",
       "       [47276, 48019],\n",
       "       [48019, 57643],\n",
       "       [57643, 46791],\n",
       "       [46791, 58573],\n",
       "       [58573, 57552],\n",
       "       [57552, 51345],\n",
       "       [51345, 50502],\n",
       "       [50502, 57342],\n",
       "       [57342,  7861],\n",
       "       [ 7861, 34136],\n",
       "       [34136, 54606],\n",
       "       [54606, 50888],\n",
       "       [50888, 41194],\n",
       "       [41194, 56296],\n",
       "       [56296, 58164],\n",
       "       [58164, 40564],\n",
       "       [40564, 40796],\n",
       "       [40796, 47952],\n",
       "       [47952, 40617],\n",
       "       [40617, 50502],\n",
       "       [50502, 41806],\n",
       "       [41806, 38921],\n",
       "       [38921, 57342],\n",
       "       [57342, 41235],\n",
       "       [41235, 34211],\n",
       "       [34211, 41235],\n",
       "       [41235, 54644],\n",
       "       [54644, 47014],\n",
       "       [47014, 57342],\n",
       "       [57342, 41454],\n",
       "       [41454, 50476],\n",
       "       [50476, 57342],\n",
       "       [57342, 59071],\n",
       "       [59071, 59461],\n",
       "       [59461, 39370],\n",
       "       [39370, 44710],\n",
       "       [44710, 38921],\n",
       "       [38921, 37563],\n",
       "       [37563, 52304],\n",
       "       [52304, 44411],\n",
       "       [44411, 57552],\n",
       "       [57552, 51345],\n",
       "       [51345, 50502],\n",
       "       [50502, 48019],\n",
       "       [48019, 41194],\n",
       "       [41194, 56296],\n",
       "       [56296, 54606],\n",
       "       [54606, 49146],\n",
       "       [49146, 52304],\n",
       "       [52304, 38921],\n",
       "       [38921, 57342],\n",
       "       [57342, 57512],\n",
       "       [57512, 50316],\n",
       "       [50316, 57552],\n",
       "       [57552, 44641],\n",
       "       [44641, 43559],\n",
       "       [43559, 51414],\n",
       "       [51414, 55170],\n",
       "       [55170,  7861],\n",
       "       [ 7861, 34136],\n",
       "       [34136, 38294],\n",
       "       [38294, 54644],\n",
       "       [54644, 48010],\n",
       "       [48010, 55720],\n",
       "       [55720, 57342],\n",
       "       [57342, 57179],\n",
       "       [57179, 54400],\n",
       "       [54400, 57552],\n",
       "       [57552, 48019],\n",
       "       [48019, 10050],\n",
       "       [10050, 21716],\n",
       "       [21716, 47091],\n",
       "       [47091, 57183],\n",
       "       [57183, 47243],\n",
       "       [47243, 38549],\n",
       "       [38549, 45908],\n",
       "       [45908, 47177],\n",
       "       [47177, 57552],\n",
       "       [57552, 37042],\n",
       "       [37042, 16842],\n",
       "       [16842, 50502],\n",
       "       [50502, 15921],\n",
       "       [15921, 34171],\n",
       "       [34171, 45539],\n",
       "       [45539, 43559],\n",
       "       [43559,  4997],\n",
       "       [ 4997, 48010],\n",
       "       [48010, 54644],\n",
       "       [54644, 48010],\n",
       "       [48010, 59461],\n",
       "       [59461, 41751],\n",
       "       [41751, 57552],\n",
       "       [57552, 39370],\n",
       "       [39370, 57342],\n",
       "       [57342, 44682],\n",
       "       [44682, 59444],\n",
       "       [59444, 48625],\n",
       "       [48625, 50502],\n",
       "       [50502, 57342],\n",
       "       [57342, 57183],\n",
       "       [57183, 45539],\n",
       "       [45539, 37042],\n",
       "       [37042, 34211],\n",
       "       [34211, 41235],\n",
       "       [41235, 54644],\n",
       "       [54644, 57342],\n",
       "       [57342, 49901],\n",
       "       [49901, 59194],\n",
       "       [59194, 51212],\n",
       "       [51212, 50502],\n",
       "       [50502, 48019],\n",
       "       [48019, 53837],\n",
       "       [53837, 51714],\n",
       "       [51714, 38428],\n",
       "       [38428, 59461],\n",
       "       [59461, 46660],\n",
       "       [46660, 51314],\n",
       "       [51314, 42283],\n",
       "       [42283, 50637],\n",
       "       [50637, 41992],\n",
       "       [41992, 38428],\n",
       "       [38428, 44322],\n",
       "       [44322, 52419],\n",
       "       [52419, 42698],\n",
       "       [42698,  7861],\n",
       "       [ 7861, 34136],\n",
       "       [34136, 49017],\n",
       "       [49017, 41375],\n",
       "       [41375, 46012],\n",
       "       [46012, 48308],\n",
       "       [48308, 45645],\n",
       "       [45645, 57044],\n",
       "       [57044, 38428],\n",
       "       [38428, 57476],\n",
       "       [57476, 52335],\n",
       "       [52335, 38428],\n",
       "       [38428, 57287],\n",
       "       [57287, 28828]], dtype=int32), values=array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.], dtype=float32), dense_shape=(59575, 59575)),\n",
       " array([ 0.13153568,  0.02592637,  0.10171928,  0.02582681,  0.12194555,\n",
       "         0.04003044,  0.13059285,  0.02247888,  0.26996028,  0.17881945,\n",
       "         0.02216886,  0.17881945,  0.07780761,  0.19272281,  0.1617115 ,\n",
       "         0.11195555,  0.12147274,  0.10604221,  0.10771536,  0.09329632,\n",
       "         0.05086713,  0.07276864,  0.16258292,  0.01819604,  0.1918343 ,\n",
       "         0.27381873,  0.2009304 ,  0.22935303,  0.14367907,  0.10780925,\n",
       "         0.05206944,  0.14136153,  0.04339334,  0.09399205,  0.11184349,\n",
       "         0.19287008,  0.02289743,  0.11123356,  0.1683497 ,  0.07258565,\n",
       "         0.18177573,  0.05642067,  0.14601032,  0.03865111,  0.20857206,\n",
       "         0.14142464,  0.03730311,  0.12805231,  0.04421996,  0.10780925,\n",
       "         0.04109742,  0.12450963,  0.2009304 ,  0.09398346,  0.04003044,\n",
       "         0.24401399,  0.2009304 ,  0.12051445,  0.14601032,  0.26996028,\n",
       "         0.21743897,  0.11235302,  0.12754472,  0.12450963], dtype=float32),\n",
       " array([ 0.18025176,  0.09583531,  0.07901362,  0.06160521,  0.16045937,\n",
       "         0.09359237,  0.10884981,  0.07901362,  0.12581429,  0.05580549,\n",
       "         0.15251043,  0.07901362,  0.06498265,  0.03852586,  0.09132485,\n",
       "         0.05657373,  0.18025176,  0.02648022,  0.12488599,  0.06969997,\n",
       "         0.12844928,  0.07730526,  0.21515408,  0.13618724,  0.12045254,\n",
       "         0.18025176,  0.12802479,  0.13734032,  0.05483261,  0.16045937,\n",
       "         0.13004158,  0.07901362,  0.09462695,  0.33588761,  0.07901362,\n",
       "         0.10076441,  0.07730526,  0.14196782,  0.1154002 ,  0.23235291,\n",
       "         0.20859231,  0.13364238,  0.05754215,  0.00400504,  0.09853138,\n",
       "         0.13062264,  0.10395138,  0.01234257,  0.13538277,  0.09935123,\n",
       "         0.07730526,  0.11402105,  0.07901362,  0.21810234,  0.12697759,\n",
       "         0.10193005,  0.23235291,  0.12581429,  0.00532031,  0.07187276,\n",
       "         0.10241427,  0.34925526,  0.13618724,  0.04649514], dtype=float32))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geo_vec_model.get_reconstruction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch-par2vec]",
   "language": "python",
   "name": "conda-env-pytorch-par2vec-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
