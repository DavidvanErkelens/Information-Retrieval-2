{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geo-Vec Model \n",
    "- basic geo-vec model\n",
    "- auxilliary task models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as ss\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.corpora.wikicorpus import WikiCorpus\n",
    "from gensim import utils\n",
    "# from scipy.sparse import coo_matrix\n",
    "\n",
    "def get_adj(tokenized_docs, word2id):\n",
    "    for docidx in tokenized_docs:\n",
    "        adj_i = np.vstack((docidx[:-1], docidx[1:]))\n",
    "        adj_o = np.flip(adj_i, axis=0)\n",
    "        sp_adj_i = ss.coo_matrix((np.ones(adj_i.shape[1]), (adj_i[0, :], adj_i[1, :])), \n",
    "                                 (len(word2id), len(word2id)))\n",
    "        sp_adj_o = ss.coo_matrix((np.ones(adj_o.shape[1]), (adj_o[0, :], adj_o[1, :])), \n",
    "                                 (len(word2id), len(word2id)))\n",
    "        yield sp_adj_o, sp_adj_i\n",
    "\n",
    "def get_lapl(tokenized_docs, word2id, renorm_trick=True):\n",
    "    for A_o, A_i in get_adj(tokenized_docs, word2id):\n",
    "        if renorm_trick == True:\n",
    "            _A_i = A_i + ss.eye(A_i.shape[0])\n",
    "            _A_o = A_o + ss.eye(A_o.shape[0])\n",
    "        D_inv_sqrt_i = ss.diags(np.power(np.array(_A_i.sum(1)), -0.5).flatten())\n",
    "        D_inv_sqrt_o = ss.diags(np.power(np.array(_A_o.sum(1)), -0.5).flatten())\n",
    "        L_i = _A_i.dot(D_inv_sqrt_i).transpose().dot(D_inv_sqrt_i).tocoo()\n",
    "        L_o = _A_o.dot(D_inv_sqrt_o).transpose().dot(D_inv_sqrt_o).tocoo()\n",
    "        \n",
    "        yield A_o, A_i, L_o, L_i\n",
    "        \n",
    "Ls = []\n",
    "for A_o, A_i, L_o, L_i in get_lapl(tokenized[:10], word2id):\n",
    "     Ls.append([A_o, A_i, L_o, L_i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.corpora.wikicorpus import WikiCorpus\n",
    "from gensim import utils\n",
    "# from scipy.sparse import coo_matrix\n",
    "\n",
    "def get_adj(tokenized_docs, word2id):\n",
    "    for docidx in tokenized_docs:\n",
    "        adj_i = np.vstack((docidx[:-1], docidx[1:]))\n",
    "        adj_o = np.flip(adj_i, axis=0)\n",
    "        sp_adj_i = ss.coo_matrix((np.ones(adj_i.shape[1]), (adj_i[0, :], adj_i[1, :])), \n",
    "                                 (len(word2id), len(word2id)))\n",
    "        sp_adj_o = ss.coo_matrix((np.ones(adj_o.shape[1]), (adj_o[0, :], adj_o[1, :])), \n",
    "                                 (len(word2id), len(word2id)))\n",
    "        yield sp_adj_o, sp_adj_i\n",
    "\n",
    "def get_lapl(tokenized_docs, word2id, renorm_trick=True):\n",
    "    for A_o, A_i in get_adj(tokenized_docs, word2id):\n",
    "        if renorm_trick == True:\n",
    "            _A_i = A_i + ss.eye(A_i.shape[0])\n",
    "            _A_o = A_o + ss.eye(A_o.shape[0])\n",
    "        D_inv_sqrt_i = ss.diags(np.power(np.array(_A_i.sum(1)), -0.5).flatten())\n",
    "        D_inv_sqrt_o = ss.diags(np.power(np.array(_A_o.sum(1)), -0.5).flatten())\n",
    "        L_i = _A_i.dot(D_inv_sqrt_i).transpose().dot(D_inv_sqrt_i).tocoo()\n",
    "        L_o = _A_o.dot(D_inv_sqrt_o).transpose().dot(D_inv_sqrt_o).tocoo()\n",
    "        \n",
    "        yield A_o, A_i, L_o, L_i\n",
    "        \n",
    "Ls = []\n",
    "for A_o, A_i, L_o, L_i in get_lapl(tokenized[:10], word2id):\n",
    "     Ls.append([A_o, A_i, L_o, L_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class WikiCorpusExtended(WikiCorpus):\n",
    "    \"\"\"\n",
    "        Extension on the WikiCorpus from gensim\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, args):\n",
    "        super().__init__(args)\n",
    "        \n",
    "    def get_docidx(self):\n",
    "        for doc in self.get_texts():\n",
    "            doc = [word if isinstance(word, str) else str(word, 'utf-8') for word in doc]\n",
    "            yield np.array([wiki.dictionary.token2id.get(word) for word in doc])\n",
    "    \n",
    "    def get_adj(self):\n",
    "        for docidx in self.get_docidx():\n",
    "            adj_i = np.vstack((docidx[:-1], docidx[1:]))\n",
    "            adj_o = np.flip(np.vstack((docidx[:-1], docidx[1:])), axis=0)\n",
    "            sp_adj_i = ss.coo_matrix((np.ones(adj_i.shape[1]), (adj_i[0, :], adj_i[1, :])), \n",
    "                                     (len(self.dictionary), len(self.dictionary)))\n",
    "            sp_adj_o = ss.coo_matrix((np.ones(adj_o.shape[1]), (adj_o[0, :], adj_o[1, :])), \n",
    "                                     (len(self.dictionary), len(self.dictionary)))\n",
    "            yield sp_adj_i, sp_adj_o\n",
    "            \n",
    "    def get_lapl(self, renorm_trick=True):\n",
    "        for A_i, A_o in self.get_adj():\n",
    "            if renorm_trick == True:\n",
    "                A_i += ss.eye(A_i.shape[0])\n",
    "                A_o += ss.eye(A_o.shape[0])\n",
    "            D_inv_sqrt_i = ss.diags(np.power(np.array(A_i.sum(1)), -0.5).flatten())\n",
    "            D_inv_sqrt_o = ss.diags(np.power(np.array(A_o.sum(1)), -0.5).flatten())\n",
    "            L_i = A_i.dot(D_inv_sqrt_i).transpose().dot(D_inv_sqrt_i).tocoo()\n",
    "            L_o = A_o.dot(D_inv_sqrt_o).transpose().dot(D_inv_sqrt_o).tocoo()\n",
    "            yield L_i, L_o\n",
    "            \n",
    "            \n",
    "# wiki = WikiCorpusExtended('process/enwiki-latest-pages-articles1.xml-p10p30302.bz2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Doc2Graph():\n",
    "    \"\"\"Convert tokenized document to weighted Adjacency matrix\n",
    "    and graph Laplacian\"\"\"\n",
    "    def __init__(self, doc, doc_id=-1):\n",
    "        self.doc = doc\n",
    "        self.doc_id = doc_id\n",
    "        \n",
    "    def doc2graph(self):\n",
    "        g = self.load()\n",
    "            \n",
    "        if not g:\n",
    "            As = self.get_As()\n",
    "            Ls = self.get_Ls(As)\n",
    "            g = As + Ls\n",
    "            self.save(g)\n",
    "\n",
    "        return g\n",
    "    \n",
    "    def get_As(self):\n",
    "        \"\"\"Get the weighted adjacency matrices of incoming\n",
    "        and outcoming edges\"\"\"\n",
    "        As = []\n",
    "        e1 = np.vstack((self.doc[:-1], self.doc[1:])).T\n",
    "        e2 = np.flip(e1, 1)\n",
    "        for a in [e2, e1]:\n",
    "            rc, cooc = np.unique(a, return_counts=True, axis=0)\n",
    "            As.append(ss.coo_matrix((cooc, (rc[:,0], rc[:,1])), \n",
    "                                   tuple((np.max(a)+1, np.max(a)+1))))\n",
    "        return As\n",
    "    \n",
    "    def get_Ls(self, As, renorm_trick=False):\n",
    "        \"\"\"Create graph Laplacians from adjacency matrices\"\"\"\n",
    "        Ls = []\n",
    "        for A in As:\n",
    "            A = ss.coo_matrix(A)\n",
    "            if renorm_trick:\n",
    "                A_ = A + ss.eye(A.shape[0])\n",
    "            D_inv_sqrt = ss.diags(np.power(np.array(A.sum(1)), -0.5).flatten())\n",
    "            L = A.dot(D_inv_sqrt).transpose().dot(D_inv_sqrt).tocoo()\n",
    "            Ls.append(L)\n",
    "            \n",
    "        return Ls    \n",
    "    \n",
    "    def save(self, g):\n",
    "        \"\"\"Save graph to folder for reuse\"\"\"\n",
    "        print('save: implement me!')\n",
    "        pass\n",
    "    \n",
    "    def load(self):\n",
    "        print('load: implement me!')\n",
    "        return None\n",
    "            \n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    if not ss.isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape\n",
    "\n",
    "def sp2tf(sp_t, shape=None):\n",
    "    t = sparse_to_tuple(sp_t)\n",
    "#     tensor = tf.SparseTensor(t[0],t[1].astype(np.float32),t[2])\n",
    "    if shape is not None:\n",
    "        t[2] == shape\n",
    "    tensor = tf.SparseTensorValue(t[0],t[1].astype(np.float32),t[2])\n",
    "    return tensor\n",
    "\n",
    "def dropout_sparse(x, keep_prob, num_nonzero_elems):\n",
    "    \"\"\"Dropout for sparse tensors. Currently fails for very large sparse tensors (>1M elements)\n",
    "    \"\"\"\n",
    "    noise_shape = [num_nonzero_elems]\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += tf.random_uniform(noise_shape)\n",
    "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "    pre_out = tf.sparse_retain(x, dropout_mask)\n",
    "    return pre_out * (1./keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geo-Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "class GeoVec():\n",
    "    def __init__(self, corpus=None, vocab_size=10, h_layers = [8, 4], \n",
    "                 act = tf.nn.relu, dropout=0.0, learning_rate = 1e-3):\n",
    "        \"\"\"Geo-Vec model as described in the report model section.\"\"\"\n",
    "        \n",
    "        self.corpus = corpus\n",
    "        self.vocab_size = vocab_size\n",
    "        self.h_layers = h_layers\n",
    "        self.act = act\n",
    "        self.dropout = dropout\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # use for plotting\n",
    "        self._loss_vals, self._acc_vals = [], []\n",
    "        \n",
    "        #placeholders\n",
    "        s = [self.vocab_size, self.vocab_size]\n",
    "        self.placeholders = {\n",
    "            'A_o': tf.sparse_placeholder(tf.float32),\n",
    "            'L_o': tf.sparse_placeholder(tf.float32),\n",
    "            'A_i': tf.sparse_placeholder(tf.float32),\n",
    "            'L_i': tf.sparse_placeholder(tf.float32),\n",
    "            'idx_i': tf.placeholder(tf.int64),\n",
    "            'idx_o': tf.placeholder(tf.int64),\n",
    "            'val_i': tf.placeholder(tf.float32),\n",
    "            'val_o': tf.placeholder(tf.float32),\n",
    "            'dropout': tf.placeholder_with_default(0., shape=())\n",
    "        }\n",
    "        \n",
    "        # model\n",
    "        self.aux_losses = None\n",
    "        dummy = sp2tf(ss.eye(self.vocab_size))\n",
    "        self.init_model(x=dummy)\n",
    "\n",
    "        #optimizer\n",
    "        self.init_optimizer()\n",
    "        \n",
    "        #sess\n",
    "        self.trained = 0\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def init_model(self, x, aux_tasks = None):\n",
    "        \"\"\"geo-vec model with variable number of gcn layers. Optional aux_taks\n",
    "        param is now unimplemented to specify which tasks to add. All aux losses\n",
    "        should be gathered in a self.aux_losses variable to gather later on.\"\"\"\n",
    "        for i, h_layer in enumerate(self.h_layers):\n",
    "            if i == 0:\n",
    "                h = self.gcn(x, self.vocab_size, self.h_layers[0], self.act, layer=i,sparse=True)  \n",
    "            elif (i+1) < len(self.h_layers):\n",
    "                h = self.gcn(h, self.h_layers[i-1], h_layer, self.act, layer=i, )\n",
    "            else:\n",
    "                self.emb_o, self.emb_i = self.gcn(h, self.h_layers[i-1], \n",
    "                                             h_layer, act=lambda x: x, layer=i,separate=True)\n",
    "                \n",
    "        # here we can left multiply the last layer h\n",
    "        # and perform auxilliary tasks.\n",
    "        posneg_samples_o = tf.gather(self.emb_o, tf.transpose(self.placeholders['idx_o']))\n",
    "        posneg_samples_i = tf.gather(self.emb_i, tf.transpose(self.placeholders['idx_i']))\n",
    "        \n",
    "        self.recon_o = self.decode(posneg_samples_o)\n",
    "        self.recon_i = self.decode(posneg_samples_i)\n",
    "    \n",
    "    def gcn(self, x, dim_in, dim_out, act, layer, sparse=False, separate=False):\n",
    "        \"\"\"basic graph convolution using a split up adjacency matrix.\n",
    "        The separation param is to create the final embeddings to reconstruct.\"\"\"\n",
    "        w1 = tf.get_variable('w1_{}'.format(layer), shape=[dim_in, dim_out], \n",
    "                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "        w2 = tf.get_variable('w2_{}'.format(layer), shape=[dim_in, dim_out], \n",
    "                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        if sparse:\n",
    "            x1 = tf.sparse_tensor_dense_matmul(x, w1)\n",
    "            x2 = tf.sparse_tensor_dense_matmul(x, w2)\n",
    "        else:\n",
    "            x1 = tf.matmul(x, w1)\n",
    "            x2 = tf.matmul(x, w2)\n",
    "            \n",
    "        x1 = tf.sparse_tensor_dense_matmul(self.placeholders['L_o'], x1)\n",
    "        x2 = tf.sparse_tensor_dense_matmul(self.placeholders['L_i'], x2)\n",
    "        \n",
    "        if separate:\n",
    "            return self.act(x1), self.act(x2)\n",
    "        \n",
    "        return self.act(x1 + x2)\n",
    "    \n",
    "    def decode(self, x, cap = 1000):\n",
    "        \"\"\"simple innerproduct decoder with sigmoid activation to scale\n",
    "        the edged between 0-1000 (assuming more co-occurances are unlikely).\"\"\"\n",
    "#         print(x)\n",
    "#         print(x.shape)\n",
    "#         a_t = x\n",
    "#         idx = tf.where(tf.not_equal(a_t, 0))\n",
    "#         # Use tf.shape(a_t, out_type=tf.int64) instead of a_t.get_shape() if tensor shape is dynamic\n",
    "#         x = tf.SparseTensor(idx, tf.gather_nd(a_t, idx), a_t.get_shape())\n",
    "        \n",
    "        x = tf.nn.dropout(x, 1-self.dropout)\n",
    "        \n",
    "#         zero = tf.constant(0, dtype=tf.float32)\n",
    "#         A_rows = tf.sparse_reduce_sum(tf.sparse_add(self.placeholders['A_o'], sp2tf(-ss.eye(self.vocab_size))), 0)\n",
    "\n",
    "#         where = tf.not_equal(A_rows, zero)\n",
    "#         indices = tf.where(where)\n",
    "#         x = tf.gather_nd(x, tf.transpose(indices))\n",
    "        \n",
    "        x = tf.reshape(tf.matmul(x, tf.transpose(x)), [-1])\n",
    "        \n",
    "\n",
    "        return tf.nn.relu(x)\n",
    "        \n",
    "    def init_optimizer(self):\n",
    "        \"\"\"initializes optimizer and computes loss + accuracy. The loss function\n",
    "        is currently a MSE, due to the fact we are dealing with weighted edges.\n",
    "        This does not seem ideal, and should be thought about.\"\"\"\n",
    "        labels_o = self.recon_o\n",
    "        labels_i = self.recon_i\n",
    "#         labels_o = tf.reshape(tf.sparse_tensor_to_dense(\n",
    "#                                 tf.gather(self.placeholders['A_i'], tf.transpose(self.placeholders['idx_i'])),\n",
    "#                                 validate_indices=False), [-1])\n",
    "#         labels_i = tf.reshape(tf.sparse_tensor_to_dense(\n",
    "#                                 tf.gather(self.placeholders['A_i'], tf.transpose(self.placeholders['idx_i'])),\n",
    "#                                 validate_indices=False), [-1])\n",
    "        \n",
    "        emb_or = tf.gather(self.emb_o, self.placeholders['idx_o'][:, 0])\n",
    "        emb_oc = tf.gather(self.emb_o, self.placeholders['idx_o'][:, 1])\n",
    "    \n",
    "        emb_ir = tf.gather(self.emb_i, self.placeholders['idx_i'][:, 0])\n",
    "        emb_ic = tf.gather(self.emb_i, self.placeholders['idx_i'][:, 1])\n",
    "        \n",
    "        self.recon_o = tf.reduce_sum(tf.multiply(emb_or, emb_oc), 1)\n",
    "        self.recon_i = tf.reduce_sum(tf.multiply(emb_ir, emb_ic), 1)\n",
    "        \n",
    "        loss_o = tf.losses.mean_squared_error(self.recon_o, self.placeholders['val_o'])\n",
    "        loss_i = tf.losses.mean_squared_error(self.recon_i, self.placeholders['val_i']) \n",
    "        self.loss = loss_o + loss_i\n",
    "        \n",
    "        # gather aux losses and add to total loss\n",
    "        if self.aux_losses:\n",
    "            self.loss += self.aux_losses\n",
    "        \n",
    "        # optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        self.opt_op = optimizer.minimize(self.loss)\n",
    "\n",
    "        cp_o = tf.equal(tf.cast(self.recon_o, tf.int32), tf.cast(self.placeholders['val_o'], tf.int32))\n",
    "        cp_i = tf.equal(tf.cast(self.recon_i, tf.int32), tf.cast(self.placeholders['val_i'], tf.int32))\n",
    "        correct_prediction = tf.concat([cp_o, cp_i], 0)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    def get_feed_dict(self, A_o, A_i, L_o, L_i, idx_i, idx_o, val_o, val_i):\n",
    "        feed_dict = {self.placeholders['A_o']: A_o,\n",
    "                     self.placeholders['A_i']: A_i,\n",
    "                     self.placeholders['L_o']: L_o,\n",
    "                     self.placeholders['L_i']: L_i,\n",
    "                     self.placeholders['idx_o']: idx_o,\n",
    "                     self.placeholders['idx_i']: idx_i,\n",
    "                     self.placeholders['val_o']: val_o,\n",
    "                     self.placeholders['val_i']: val_i}\n",
    "        return feed_dict\n",
    "    \n",
    "    def get_sample(self, batch_size=64, ratio=1.0):\n",
    "        \"\"\"get random sample from corpus graph cache\"\"\"\n",
    "        dummy = random.choice(Ls).copy()\n",
    "        \n",
    "        pos_idx_o = np.random.choice(range(len(dummy[0].row)), batch_size)\n",
    "        pos_idx_i = np.random.choice(range(len(dummy[1].row)), batch_size)\n",
    "        \n",
    "        idx_o = np.array(list(zip(dummy[0].row, dummy[0].col)))[pos_idx_o, :]\n",
    "        idx_i = np.array(list(zip(dummy[1].row, dummy[1].col)))[pos_idx_i, :]\n",
    "        val_o = dummy[0].data[pos_idx_o]\n",
    "        val_i = dummy[1].data[pos_idx_i]\n",
    "        \n",
    "        for i, d in enumerate(dummy):\n",
    "            dummy[i] = sp2tf(d)\n",
    "\n",
    "        return dummy, idx_o, idx_i, val_o, val_i\n",
    "    \n",
    "    def train(self, num_epochs = 100, print_freq=50):\n",
    "        \"\"\"train op that can be invoked multiple times.\"\"\"\n",
    "        tf.set_random_seed(42)\n",
    "        np.random.seed(42)\n",
    "\n",
    "        for e in range(num_epochs):\n",
    "            self.trained += 1\n",
    "            (A_o, A_i, L_o, L_i), idx_o, idx_i, val_o, val_i = self.get_sample()\n",
    "            \n",
    "            feed_dict = self.get_feed_dict(A_o, A_i, L_o, L_i, idx_o, idx_i, val_o, val_i)\n",
    "            \n",
    "#             idx = np.random.choice(self.placeholders['A_o'].indices[:,0], size=(10,1))\n",
    "#             idx = tf.multinomial(self.placeholders['A_o'].indices[:,0])\n",
    "#             pos_idx = np.random.choice(idx)\n",
    "#             x = tf.sparse_slice(self.placeholders['A_o'], self.placeholders['A_o'].indices[:,0], tf.ones(self.placeholders['A_o'].indices[:,0].shape[1]))\n",
    "#             x = tf.gather(self.emb_o, idx_o)\n",
    "#             o = self.sess.run([x], feed_dict=feed_dict)\n",
    "            \n",
    "            outs = self.sess.run([self.opt_op, self.loss, self.accuracy], feed_dict=feed_dict)\n",
    "            avg_loss, avg_acc = outs[1], outs[2]\n",
    "            self._loss_vals.append(avg_loss)\n",
    "            self._acc_vals.append(avg_acc)\n",
    "            \n",
    "            print('\\r epoch: %d/%d \\t loss: %.3f \\t avg_acc: %.3f' \n",
    "                      % (e+1, num_epochs, avg_loss, avg_acc), end='')\n",
    "            if (e + 1) % print_freq == 0:\n",
    "                print('')\n",
    "        else:\n",
    "            print('----> done training: {} epochs'.format(self.trained))\n",
    "        \n",
    "    def plot(self):\n",
    "        \"\"\"Plotting loss function\"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(self._loss_vals, color='red')\n",
    "        plt.plot(self._acc_vals, color='blue')\n",
    "        \n",
    "        plt.legend(handles=[mpatches.Patch(color='red', label='loss'),\n",
    "                            mpatches.Patch(color='blue', label='acc')],\n",
    "                   bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "        plt.show()\n",
    "        \n",
    "    def get_reconstruction(self, doc = None):\n",
    "        if doc:\n",
    "            A_o, A_i, L_o, L_i = Doc2Graph(doc, doc_id).doc2graph()\n",
    "        else:\n",
    "            (A_o, A_i, L_o, L_i), idx_o, idx_i, val_o, val_i = self.get_sample()\n",
    "#             A_o, A_i, L_o, L_i = self.get_sample()\n",
    "            \n",
    "        feed_dict = self.get_feed_dict(A_o, A_i, L_o, L_i, idx_o, idx_i, val_o, val_i)\n",
    "#         feed_dict = self.get_feed_dict(A_o, A_i, L_o, L_i)\n",
    "        recon_o, recon_i = self.sess.run([self.recon_o, self.recon_i], feed_dict=feed_dict)\n",
    "        return A_o, A_i, recon_o, recon_i\n",
    "    \n",
    "    def get_embeddings(self, doc = None, doc_id = None):\n",
    "        if doc:\n",
    "            A_o, A_i, L_o, L_i = Doc2Graph(doc, doc_id).doc2graph()\n",
    "        else:\n",
    "            (A_o, A_i, L_o, L_i), idx_o, idx_i, val_o, val_i = self.get_sample()\n",
    "#             A_o, A_i, L_o, L_i = self.get_sample()\n",
    "            \n",
    "        feed_dict = self.get_feed_dict(A_o, A_i, L_o, L_i, idx_o, idx_i, val_o, val_i)\n",
    "        \n",
    "#         feed_dict = self.get_feed_dict(A_o, A_i, L_o, L_i, )\n",
    "        emb_o, emb_i = self.sess.run([self.emb_o, self.emb_i], feed_dict=feed_dict)\n",
    "        return A_o, A_i, emb_o, emb_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# fake_doc = np.asarray([1, 2, 3, 4, 1, 5, 6, 4, 1, 7, 9])\n",
    "geo_vec_model = GeoVec(vocab_size=len(word2id), h_layers = [6, 4])\n",
    "geo_vec_model.train(100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "geo_vec_model.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "geo_vec_model.get_reconstruction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch-par2vec]",
   "language": "python",
   "name": "conda-env-pytorch-par2vec-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
