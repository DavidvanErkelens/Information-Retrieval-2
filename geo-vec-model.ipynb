{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geo-Vec Model \n",
    "- basic geo-vec model\n",
    "- auxilliary task models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as ss\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Doc2Graph():\n",
    "    \"\"\"Convert tokenized document to weighted Adjacency matrix\n",
    "    and graph Laplacian\"\"\"\n",
    "    def __init__(self, doc, doc_id=-1):\n",
    "        self.doc = doc\n",
    "        self.doc_id = doc_id\n",
    "        \n",
    "    def doc2graph(self):\n",
    "        g = self.load()\n",
    "            \n",
    "        if not g:\n",
    "            As = self.get_As()\n",
    "            Ls = self.get_Ls(As)\n",
    "            g = As + Ls\n",
    "            self.save(g)\n",
    "\n",
    "        return g\n",
    "    \n",
    "    def get_As(self):\n",
    "        \"\"\"Get the weighted adjacency matrices of incoming\n",
    "        and outcoming edges\"\"\"\n",
    "        As = []\n",
    "        e1 = np.vstack((self.doc[:-1], self.doc[1:])).T\n",
    "        e2 = np.flip(e1, 1)\n",
    "        for a in [e2, e1]:\n",
    "            rc, cooc = np.unique(a, return_counts=True, axis=0)\n",
    "            As.append(ss.coo_matrix((cooc, (rc[:,0], rc[:,1])), \n",
    "                                   tuple((np.max(a)+1, np.max(a)+1))))\n",
    "        return As\n",
    "    \n",
    "    def get_Ls(self, As, renorm_trick=True):\n",
    "        \"\"\"Create graph Laplacians from adjacency matrices\"\"\"\n",
    "        Ls = []\n",
    "        for A in As:\n",
    "            A = ss.coo_matrix(A)\n",
    "            if renorm_trick:\n",
    "                A += ss.eye(A.shape[0])\n",
    "            D_inv_sqrt = ss.diags(np.power(np.array(A.sum(1)), -0.5).flatten())\n",
    "            L = A.dot(D_inv_sqrt).transpose().dot(D_inv_sqrt).tocoo()\n",
    "            Ls.append(L)\n",
    "            \n",
    "        return Ls\n",
    "    \n",
    "    def save(self, g):\n",
    "        \"\"\"Save graph to folder for reuse\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def load(self):\n",
    "        return None\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    if not ss.isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape\n",
    "\n",
    "def sp2tf(sp_t, shape=None):\n",
    "    t = sparse_to_tuple(sp_t)\n",
    "    if shape:\n",
    "        t = tuple((t[0],t[1],shape))\n",
    "    tensor = tf.SparseTensorValue(t[0],t[1].astype(np.float32),t[2])\n",
    "    return tensor\n",
    "\n",
    "def tfs2sp(tfs):\n",
    "    return ss.coo_matrix((tfs[1], (tfs[0][:,0], tfs[0][:,1])), tfs[2])\n",
    "    \n",
    "def dropout_sparse(x, keep_prob, num_nonzero_elems):\n",
    "    \"\"\"Dropout for sparse tensors. Currently fails for very large sparse tensors (>1M elements)\n",
    "    \"\"\"\n",
    "    noise_shape = [num_nonzero_elems]\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += tf.random_uniform(noise_shape)\n",
    "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "    pre_out = tf.sparse_retain(x, dropout_mask)\n",
    "    return pre_out * (1./keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 4]\n",
      " [2 3]\n",
      " [3 2]\n",
      " [4 1]]\n",
      "[[1 4]\n",
      " [2 3]\n",
      " [3 2]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1, 2, 3, 4],[4, 3, 2, 1]]).T\n",
    "#idx = np.random.randint(np.shape(a)[0], size = [np.shape(a)[0], 1])\n",
    "#np.squeeze(a[idx])\n",
    "print(a)\n",
    "b = np.array([[1, 2, 3],[4, 3, 2]]).T\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geo-Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GeoVec():\n",
    "    def __init__(self, corpus=None, vocab_size=10, h_layers = [8, 4], \n",
    "                 act = tf.nn.relu, dropout=0.0, learning_rate = 1e-3):\n",
    "        \"\"\"Geo-Vec model as described in the report model section.\"\"\"\n",
    "        \n",
    "        self.corpus = corpus\n",
    "        self.vocab_size = vocab_size\n",
    "        self.h_layers = h_layers\n",
    "        self.act = act\n",
    "        self.dropout = dropout\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # use for plotting\n",
    "        self._loss_vals, self._acc_vals = [], []\n",
    "        \n",
    "        #placeholders\n",
    "        s = [self.vocab_size, self.vocab_size]\n",
    "        self.placeholders = {\n",
    "            'A_o': tf.sparse_placeholder(tf.float32),\n",
    "            'L_o': tf.sparse_placeholder(tf.float32),\n",
    "            'A_i': tf.sparse_placeholder(tf.float32),\n",
    "            'L_i': tf.sparse_placeholder(tf.float32),\n",
    "            'dropout': tf.placeholder_with_default(0., shape=())\n",
    "        }\n",
    "        \n",
    "        # model\n",
    "        self.aux_losses = None\n",
    "        dummy = sp2tf(ss.eye(self.vocab_size))\n",
    "        self.init_model(x=dummy)\n",
    "\n",
    "        #optimizer\n",
    "        self.sample_mesh = np.mgrid[0:self.vocab_size,0:self.vocab_size].reshape(2,-1).T\n",
    "        self.init_optimizer()\n",
    "        \n",
    "        #sess\n",
    "        self.trained = 0\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def init_model(self, x, num_topics = 10, aux_tasks = None):\n",
    "        \"\"\"geo-vec model with variable number of gcn layers. Optional aux_taks\n",
    "        param is now unimplemented to specify which tasks to add. All aux losses\n",
    "        should be gathered in a self.aux_losses variable to gather later on.\"\"\"\n",
    "        for i, h_layer in enumerate(self.h_layers):\n",
    "            if i == 0:\n",
    "                h = self.gcn(x, self.vocab_size, self.h_layers[0], self.act, layer=i,sparse=True)  \n",
    "            elif (i+1) < len(self.h_layers):\n",
    "                h = self.gcn(h, self.h_layers[i-1], h_layer, self.act, layer=i, )\n",
    "        else:\n",
    "            self.emb_o, self.emb_i = self.gcn(h, self.h_layers[i-1], \n",
    "                                         h_layer, act=lambda x: x, layer=-1,separate=True)\n",
    "        \n",
    "        # here we can left multiply the last layer h\n",
    "        # and perform auxilliary tasks.\n",
    "#         w_a = tf.get_variable('w_a', shape=[num_topics, self.vocab_size], \n",
    "#                              initializer=tf.truncated_normal_initializer(0.,1e-3))\n",
    "#         z_n = tf.matmul(w_a, h)\n",
    "\n",
    "        #self.pos_o, self.pos_i, self.neg_o, self.neg_i = self.sample_pos_neg()\n",
    "        \n",
    "    \n",
    "        self.recon_o = self.decode(self.emb_o, rbf=True)\n",
    "        self.recon_i = self.decode(self.emb_i, rbf=True)\n",
    "    \n",
    "    def gcn(self, x, dim_in, dim_out, act, layer, sparse=False, separate=False):\n",
    "        \"\"\"basic graph convolution using a split up adjacency matrix.\n",
    "        The separation param is to create the final embeddings to reconstruct.\"\"\"\n",
    "        w1 = tf.get_variable('w1_{}'.format(layer), shape=[dim_in, dim_out], \n",
    "                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "        w2 = tf.get_variable('w2_{}'.format(layer), shape=[dim_in, dim_out], \n",
    "                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "        if sparse:\n",
    "            x1 = tf.sparse_tensor_dense_matmul(x, w1)\n",
    "            x2 = tf.sparse_tensor_dense_matmul(x, w2)\n",
    "        else:\n",
    "            x1 = tf.matmul(x, w1)\n",
    "            x2 = tf.matmul(x, w2)\n",
    "            \n",
    "        x1 = tf.sparse_tensor_dense_matmul(self.placeholders['L_o'], x1)\n",
    "        x2 = tf.sparse_tensor_dense_matmul(self.placeholders['L_i'], x2)\n",
    "        \n",
    "        if separate:\n",
    "            return self.act(x1), self.act(x2)\n",
    "        \n",
    "        return self.act(x1 + x2)\n",
    "    \n",
    "    def decode(self, x, rbf=False):\n",
    "        \"\"\"simple innerproduct decoder with positive activation to scale\n",
    "        the edged between 0-> (assuming more co-occurances are unlikely).\"\"\"\n",
    "        if rbf:\n",
    "            return self.decode_RBF(x)\n",
    "        x = tf.nn.dropout(x, 1-self.dropout)\n",
    "        x = tf.reshape(tf.matmul(x, tf.transpose(x)), [-1])\n",
    "        return tf.nn.relu(x)\n",
    "      \n",
    "    def decode_RBF(self, x):\n",
    "        \"\"\"RBF decoder\"\"\"\n",
    "        dim = tf.shape(x)[0]\n",
    "        x2 = tf.reshape(tf.tile(tf.square(tf.norm(x, axis=1)), [dim]), [dim, dim])\n",
    "        pairwise_dist2 = x2 + tf.transpose(x2) - 2*tf.matmul(x, tf.transpose(x))\n",
    "        x = tf.exp(-pairwise_dist2/(2.*(0.5**2)))\n",
    "        x = tf.reshape(x, [-1])\n",
    "        return tf.nn.softmax(x)\n",
    "        \n",
    "    def init_optimizer(self):\n",
    "        \"\"\"initializes optimizer and computes loss + accuracy. The loss function\n",
    "        is currently a MSE, due to the fact we are dealing with weighted edges.\n",
    "        This does not seem ideal, and should be thought about.\"\"\"\n",
    "        labels_o = tf.reshape(tf.sparse_tensor_to_dense(\n",
    "                                self.placeholders['A_o'], validate_indices=False), [-1])\n",
    "        labels_i = tf.reshape(tf.sparse_tensor_to_dense(\n",
    "                                self.placeholders['A_i'], validate_indices=False), [-1])\n",
    "        \n",
    "        loss_o = tf.losses.mean_squared_error(self.recon_o, labels_o, \n",
    "                                              weights = self.get_weights(labels_o))\n",
    "        loss_i = tf.losses.mean_squared_error(self.recon_i, labels_i, \n",
    "                                              weights = self.get_weights(labels_i)) \n",
    "        self.loss = loss_o + loss_i\n",
    "        \n",
    "        # gather aux losses and add to total loss\n",
    "        if self.aux_losses:\n",
    "            self.loss += self.aux_losses\n",
    "        \n",
    "        # optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        self.opt_op = optimizer.minimize(self.loss)\n",
    "\n",
    "        cp_o = tf.equal(tf.cast(self.recon_o, tf.int32), tf.cast(labels_o, tf.int32))\n",
    "        cp_i = tf.equal(tf.cast(self.recon_i, tf.int32), tf.cast(labels_i, tf.int32))\n",
    "        correct_prediction = tf.concat([cp_o, cp_i], 0)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    def get_weights(self, labels):\n",
    "        \"\"\"Compute positive and negative example weights\"\"\"\n",
    "        pos_sum = tf.reduce_sum(labels)\n",
    "        pos_weight = (self.vocab_size**2 - pos_sum) / pos_sum\n",
    "        thresh = tf.fill(tf.shape(labels), 1.)\n",
    "        x = tf.fill(tf.shape(labels), 0.001)\n",
    "        y = tf.fill(tf.shape(labels), pos_weight)\n",
    "        return tf.where(tf.less(labels, thresh), x, y)\n",
    "    \n",
    "    def get_feed_dict(self, A_o, A_i, L_o, L_i):\n",
    "        feed_dict = {self.placeholders['A_o']: A_o,\n",
    "                     self.placeholders['A_i']: A_i,\n",
    "                     self.placeholders['L_o']: L_o,\n",
    "                     self.placeholders['L_i']: L_i}\n",
    "        return feed_dict\n",
    "    \n",
    "    def get_sample(self):\n",
    "        \"\"\"get random sample from corpus graph cache\"\"\"\n",
    "        dummy = Doc2Graph(fake_doc).doc2graph()\n",
    "        for i, d in enumerate(dummy):\n",
    "            dummy[i] = sp2tf(d, tuple((self.vocab_size, self.vocab_size)))\n",
    "        return dummy\n",
    "    \n",
    "    def train(self, num_epochs = 100, print_freq=50):\n",
    "        \"\"\"train op that can be invoked multiple times.\"\"\"\n",
    "        tf.set_random_seed(42)\n",
    "        np.random.seed(42)\n",
    "\n",
    "        for e in range(num_epochs):\n",
    "            self.trained += 1\n",
    "            A_o, A_i, L_o, L_i = self.get_sample()\n",
    "            feed_dict = self.get_feed_dict(A_o, A_i, L_o, L_i)\n",
    "    \n",
    "            outs = self.sess.run([self.opt_op, self.loss, self.accuracy], feed_dict=feed_dict)\n",
    "            avg_loss, avg_acc = outs[1], outs[2]\n",
    "            self._loss_vals.append(avg_loss)\n",
    "            self._acc_vals.append(avg_acc)\n",
    "            \n",
    "            print('\\r epoch: %d/%d \\t loss: %.3f \\t avg_acc: %.3f' \n",
    "                      % (e+1, num_epochs, avg_loss, avg_acc), end='')\n",
    "            if (e + 1) % print_freq == 0:\n",
    "                print('')\n",
    "        else:\n",
    "            print('----> done training: {} epochs'.format(self.trained))\n",
    "        \n",
    "    def plot(self):\n",
    "        \"\"\"Plotting loss function\"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(self._loss_vals, color='red')\n",
    "        plt.plot(self._acc_vals, color='blue')\n",
    "        \n",
    "        plt.legend(handles=[mpatches.Patch(color='red', label='loss'),\n",
    "                            mpatches.Patch(color='blue', label='acc')],\n",
    "                   bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "        plt.show()\n",
    "        \n",
    "    def get_reconstruction(self, doc = None):\n",
    "        if doc:\n",
    "            A_o, A_i, L_o, L_i = Doc2Graph(doc, doc_id).doc2graph()\n",
    "        else:\n",
    "            A_o, A_i, L_o, L_i = self.get_sample()\n",
    "            \n",
    "        feed_dict = self.get_feed_dict(A_o, A_i, L_o, L_i)\n",
    "        recon_o, recon_i = self.sess.run([self.recon_o, self.recon_i], feed_dict=feed_dict)\n",
    "        return A_o, A_i, recon_o, recon_i\n",
    "    \n",
    "    def get_embeddings(self, doc = None, doc_id = None):\n",
    "        if doc:\n",
    "            A_o, A_i, L_o, L_i = Doc2Graph(doc, doc_id).doc2graph()\n",
    "        else:\n",
    "            A_o, A_i, L_o, L_i = self.get_sample()\n",
    "            \n",
    "        feed_dict = self.get_feed_dict(A_o, A_i, L_o, L_i)\n",
    "        emb_o, emb_i = self.sess.run([self.em_o, self.emb_i], feed_dict=feed_dict)\n",
    "        return A_o, A_i, emb_o, emb_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch: 300/2500 \t loss: 2.099 \t avg_acc: 0.840\n",
      " epoch: 600/2500 \t loss: 2.096 \t avg_acc: 0.840\n",
      " epoch: 900/2500 \t loss: 2.094 \t avg_acc: 0.840\n",
      " epoch: 1200/2500 \t loss: 2.093 \t avg_acc: 0.840\n",
      " epoch: 1500/2500 \t loss: 2.091 \t avg_acc: 0.840\n",
      " epoch: 1800/2500 \t loss: 2.090 \t avg_acc: 0.840\n",
      " epoch: 2100/2500 \t loss: 2.088 \t avg_acc: 0.840\n",
      " epoch: 2400/2500 \t loss: 2.087 \t avg_acc: 0.840\n",
      " epoch: 2500/2500 \t loss: 2.086 \t avg_acc: 0.840----> done training: 2500 epochs\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "fake_doc = np.asarray([1, 0, 1, 2, 1, 0])\n",
    "geo_vec_model = GeoVec(vocab_size=5, h_layers = [4,2], learning_rate = 1e-4)\n",
    "geo_vec_model.get_sample()\n",
    "geo_vec_model.train(2500, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxsAAAFpCAYAAAD0qzUqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nH8hkFwB7yLAYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b6afb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "geo_vec_model.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 0],\n",
       "       [1, 2],\n",
       "       [2, 1]], dtype=int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1, a2, r1, r2 = geo_vec_model.get_reconstruction()\n",
    "a1.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  2.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfs2sp(a1).A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(r1, (geo_vec_model.vocab_size,-1)).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparseTensorValue(indices=array([[0, 0]]), values=array([ 2.], dtype=float32), dense_shape=array([1, 1]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'SparseSlice_12:1' shape=(?,) dtype=float32>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sp = tf.sparse_slice(a1,[0,1], [1,1])\n",
    "print(sp.eval())\n",
    "sess.close()\n",
    "sp.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = 1.0 / tf.constant([[1., 1., 1., 1., 1.],\n",
    "                 [1., 1., 1., 1., 1.],\n",
    "                 [1., 1., 1., 1., 1.],\n",
    "                 [1., 1., 1., 1., 1.],\n",
    "                 [1., 1., 1., 1., 1.]])\n",
    "sp.__div__(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aa, a2, l1, l2 = Doc2Graph(fake_doc).doc2graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a1.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_idx = a1.indices[np.random.choice(np.arange(np.shape(a1)[0]), size=[]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_idx = a1.indices[np.random.randint(np.shape(a1.indices)[0], size=[4,1])].squeeze()\n",
    "neg_idx = np.random.randint(10, size=[4,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1],\n",
       "       [2, 1],\n",
       "       [3, 1],\n",
       "       [1, 2],\n",
       "       [2, 2],\n",
       "       [3, 2],\n",
       "       [1, 3],\n",
       "       [2, 3],\n",
       "       [3, 3]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2,3])\n",
    "b = np.array([1, 2])\n",
    "np.vstack(np.meshgrid(a, a)).reshape(2, -1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch3]",
   "language": "python",
   "name": "conda-env-pytorch3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
