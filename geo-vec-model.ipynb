{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geo-Vec Model \n",
    "- basic geo-vec model\n",
    "- auxilliary task models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as ss\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Doc2Graph():\n",
    "    \"\"\"Convert tokenized document to weighted Adjacency matrix\n",
    "    and graph Laplacian\"\"\"\n",
    "    def __init__(self, doc, doc_id):\n",
    "        self.doc = doc\n",
    "        self.doc_id = doc_id\n",
    "        \n",
    "    def doc2graph(self):\n",
    "        g = self.load(self.doc_id)\n",
    "            \n",
    "        if not g:\n",
    "            As = self.get_A(self.doc)\n",
    "            Ls = self.get_L(As)\n",
    "            g = As.append(Ls)\n",
    "            self.save(g)\n",
    "        \n",
    "        return g\n",
    "    \n",
    "    def get_As(self):\n",
    "        \"\"\"Get the weighted adjacency matrices of incoming\n",
    "        and outcoming edges\"\"\"\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def get_L(self, As, renorm_trick=False):\n",
    "        \"\"\"Create graph Laplacians from adjacency matrices\"\"\"\n",
    "        Ls = []\n",
    "        for A in As:\n",
    "            A = sp.coo_matrix(A)\n",
    "            if renorm_trick:\n",
    "                A += sp.eye(A.shape[0])\n",
    "            D_inv_sqrt = sp.diags(np.power(np.array(A.sum(1)), -0.5).flatten())\n",
    "            L = A.dot(D_inv_sqrt).transpose().dot(D_inv_sqrt).tocoo()\n",
    "            Ls.append(L)\n",
    "            \n",
    "        return Ls\n",
    "    \n",
    "    def save(self, g):\n",
    "        \"\"\"Save graph to folder for reuse\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def load(self):\n",
    "        pass\n",
    "            \n",
    "        \n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    if not ss.isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape\n",
    "\n",
    "def sp2tf(sp_t):\n",
    "    t = sparse_to_tuple(sp_t)\n",
    "    tensor = tf.SparseTensor(t[0],t[1].astype(np.float32),t[2])\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert to double adjaceny matrices experiments\n",
    "# d = np.asarray([8, 5, 6, 3, 8, 5, 2, 1], np.newaxis)\n",
    "# e = np.vstack((np.reshape(d, (-1, 2)),np.reshape(d[1:-1], (-1, 2))))  \n",
    "# np.flip(e, 1)\n",
    "# e[np.lexsort((e[:,1], e[:,0]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geo-Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GeoVec():\n",
    "    def __init__(self, corpus=None, vocab_size=10, h_layers = [64, 32], \n",
    "                 act = tf.nn.relu, dropout=0.0, learning_rate = 1e-3):\n",
    "        \"\"\"Geo-Vec model as described in the report model section.\"\"\"\n",
    "        \n",
    "        self.corpus = corpus\n",
    "        self.vocab_size = vocab_size\n",
    "        self.h_layers = h_layers\n",
    "        self.act = act\n",
    "        self.dropout = dropout\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # use for plotting\n",
    "        self._loss_vals, self._acc_vals = [], []\n",
    "        \n",
    "        #placeholders\n",
    "        self.placeholders = {\n",
    "            'Ao': tf.sparse_placeholder(tf.float32),\n",
    "            'Lo': tf.sparse_placeholder(tf.float32),\n",
    "            'Ai': tf.sparse_placeholder(tf.float32),\n",
    "            'Li': tf.sparse_placeholder(tf.float32),\n",
    "            'dropout': tf.placeholder_with_default(0., shape=())\n",
    "        }\n",
    "        \n",
    "        # model\n",
    "        self.aux_losses = None\n",
    "        self.init_model(x=sp2tf(ss.eye(self.vocab_size)))\n",
    "\n",
    "        #optimizer\n",
    "        self.init_optimizer()\n",
    "        \n",
    "        #sess\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def init_model(self, x, aux_tasks = None):\n",
    "        \"\"\"geo-vec model with variable number of gcn layers. Optional aux_taks\n",
    "        param is now unimplemented to specify which tasks to add. All aux losses\n",
    "        should be gathered in a self.aux_losses variable to gather later on.\"\"\"\n",
    "        for i, h_layer in enumerate(self.h_layers):\n",
    "            if i == 0:\n",
    "                h = self.gcn(x, self.vocab_size, self.h_layers[0], self.act)  \n",
    "            elif (i+1) < len(self.h_layers):\n",
    "                h = self.gcn(h, self.h_layers[i-1], h_layer,self.act)\n",
    "            else:\n",
    "                self.emb_o, self.emb_i = self.gcn(h, self.h_layers[i-1], \n",
    "                                             h_layer, act=lambda x: x, separate=True)\n",
    "        \n",
    "        # here we can left multiply the last layer h\n",
    "        # and perform auxilliary tasks.\n",
    "        \n",
    "        self.recon_o = self.decode(self.emb_o)\n",
    "        self.recon_i = self.decode(self.emb_i)\n",
    "    \n",
    "    def gcn(self, x, dim_in, dim_out, act, separate=False):\n",
    "        \"\"\"basic graph convolution using a split up adjacency matrix.\n",
    "        The separation param is to create the final embeddings to reconstruct.\"\"\"\n",
    "        w1 = tf.get_variable('w1', shape=[dim_in, dim_out], \n",
    "                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "        w2 = tf.get_variable('w2', shape=[dim_in, dim_out], \n",
    "                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "        x1 = tf.matmul(tf.nn.dropout(x, 1-self.dropout), w1)\n",
    "        x2 = tf.matmul(tf.nn.dropout(x, 1-self.dropout), w2)\n",
    "        x1 = tf.sparse_tensor_dense_matmul(self.placeholders['L1'], x)\n",
    "        x2 = tf.sparse_tensor_dense_matmul(self.placeholders['L2'], x)\n",
    "        \n",
    "        if separate:\n",
    "            return self.act(x1), self.act(x2)\n",
    "        \n",
    "        return self.act(x1 + x2)\n",
    "    \n",
    "    def decode(self, x, cap = 1000):\n",
    "        \"\"\"simple innerproduct decoder with sigmoid activation to scale\n",
    "        the edged between 0-1000 (assuming more co-occurances are unlikely).\"\"\"\n",
    "        x = tf.nn.dropout(x, 1-self.dropout)\n",
    "        x = tf.reshape(tf.matmul(tf.transpose(x), x), [-1])\n",
    "        return tf.sigmoid(x)*cap\n",
    "        \n",
    "    def init_optimizer(self):\n",
    "        \"\"\"initializes optimizer and computes loss + accuracy. The loss function\n",
    "        is currently a MSE, due to the fact we are dealing with weighted edges.\n",
    "        This does not seem ideal, and should be thought about.\"\"\"\n",
    "        \n",
    "        labels_o = tf.reshape(tf.sparse_tensor_to_dense(\n",
    "                                self.placeholders['A_o'],validate_indices=False), [-1])\n",
    "        labels_i = tf.reshape(tf.sparse_tensor_to_dense(\n",
    "                                self.placeholders['A_i'],validate_indices=False), [-1])\n",
    "        loss_o = tf.losses.mean_squared_error(self.recon_o, labels_o)\n",
    "        loss_i = tf.losses.mean_squared_error(self.recon_i, labels_i) \n",
    "        self.loss = loss_o + loss_i\n",
    "        \n",
    "        # gather aux losses and add to total loss\n",
    "        if self.aux_losses:\n",
    "            self.loss += self.aux_losses\n",
    "        \n",
    "        # optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        self.opt_op = optimizer.minimize(self.loss)\n",
    "\n",
    "        cp_o = tf.equal(tf.cast(tf.greater_equal(self.recap_o, 0.5), tf.int32),\n",
    "                                                   tf.cast(labels_o, tf.int32))\n",
    "        cp_i = tf.equal(tf.cast(tf.greater_equal(self.recap_i, 0.5), tf.int32),\n",
    "                                                   tf.cast(labels_i, tf.int32))\n",
    "        \n",
    "        correct_prediction = tf.concat([cp_o, cp_i], 1)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    def get_feed_dict(self):\n",
    "        feed_dict = dict()\n",
    "        feed_dict.update({placeholders['A_o']: A_o})\n",
    "        feed_dict.update({placeholders['A_i']: A_i})\n",
    "        feed_dict.update({placeholders['L_o']: L_o})\n",
    "        feed_dict.update({placeholders['L_i']: L_i})\n",
    "        return feed_dict\n",
    "    \n",
    "    def get_sample():\n",
    "        \"\"\"get random sample from corpus graph cache\"\"\"\n",
    "        dummy = [] \n",
    "        for d in dummy:\n",
    "            d.append(sp2tf(sp2tf(ss.eye(self.vocab_size))))\n",
    "\n",
    "        return dummy\n",
    "    \n",
    "    def train(self, num_epochs = 100, print_freq=50):\n",
    "        \"\"\"train op that can be invoked multiple times.\"\"\"\n",
    "        tf.set_random_seed(42)\n",
    "        np.random.seed(42)\n",
    "\n",
    "        for e in range(num_epochs):\n",
    "            A_o, A_i, L_o, L_i = self.get_sample()\n",
    "            self.feed_dict = get_feed_dict(A_o, A_i, L_o, L_i)\n",
    "    \n",
    "            outs = self.sess.run([self.opt_op, self.loss, \n",
    "                                  self.accuracy], feed_dict=self.feed_dict)\n",
    "            avg_loss, avg_acc = outs[1], outs[2]\n",
    "            self._los_vals.append(avg_loss)\n",
    "            self._acc_vals.append(avg_acc)\n",
    "            \n",
    "            if (e + 1) % print_freq == 0:\n",
    "                print('\\r epoch: %d/%d \\t loss: %.3f \\t avg_acc: %.3f' \n",
    "                      % (e+1, num_epochs, avg_loss, avg_acc), end='')\n",
    "        \n",
    "    \n",
    "    def plot(self):\n",
    "        \"\"\"Plotting loss function\"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(self._loss_vals, color='red')\n",
    "        plt.plot(self._acc_vals, color='blue')\n",
    "        \n",
    "        plt.legend(handles=[mpatches.Patch(color='red', label='loss'),\n",
    "                            mpatches.Patch(color='blue', label='acc')],\n",
    "                   bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "geo_vec_model = GeoVec()\n",
    "geo_vec_model.train(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch3]",
   "language": "python",
   "name": "conda-env-pytorch3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
