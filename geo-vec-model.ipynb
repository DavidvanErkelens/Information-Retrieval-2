{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geo-Vec Model \n",
    "- basic geo-vec model\n",
    "- auxilliary task models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as ss\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Doc2Graph():\n",
    "    \"\"\"Convert tokenized document to weighted Adjacency matrix\n",
    "    and graph Laplacian\"\"\"\n",
    "    def __init__(self, doc, doc_id):\n",
    "        self.doc = doc\n",
    "        self.doc_id = doc_id\n",
    "        \n",
    "    def doc2graph(self):\n",
    "        g = self.load(self.doc_id)\n",
    "            \n",
    "        if not g:\n",
    "            As = self.get_A(self.doc)\n",
    "            Ls = self.get_L(As)\n",
    "            g = As.append(Ls)\n",
    "            self.save(g)\n",
    "        \n",
    "        return g\n",
    "    \n",
    "    def get_As(self):\n",
    "        \"\"\"Get the weighted adjacency matrices of incoming\n",
    "        and outcoming edges\"\"\"\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def get_L(self, As, renorm_trick=False):\n",
    "        \"\"\"Create graph Laplacians from adjacency matrices\"\"\"\n",
    "        Ls = []\n",
    "        for A in As:\n",
    "            A = sp.coo_matrix(A)\n",
    "            if renorm_trick:\n",
    "                A += sp.eye(A.shape[0])\n",
    "            D_inv_sqrt = sp.diags(np.power(np.array(A.sum(1)), -0.5).flatten())\n",
    "            L = A.dot(D_inv_sqrt).transpose().dot(D_inv_sqrt).tocoo()\n",
    "            Ls.append(L)\n",
    "            \n",
    "        return Ls\n",
    "    \n",
    "    def save(self, g):\n",
    "        \"\"\"Save graph to folder for reuse\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def load(self):\n",
    "        pass\n",
    "            \n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    if not ss.isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape\n",
    "\n",
    "def sp2tf(sp_t):\n",
    "    t = sparse_to_tuple(sp_t)\n",
    "#     tensor = tf.SparseTensor(t[0],t[1].astype(np.float32),t[2])\n",
    "    tensor = tf.SparseTensorValue(t[0],t[1].astype(np.float32),t[2])\n",
    "    return tensor\n",
    "\n",
    "def dropout_sparse(x, keep_prob, num_nonzero_elems):\n",
    "    \"\"\"Dropout for sparse tensors. Currently fails for very large sparse tensors (>1M elements)\n",
    "    \"\"\"\n",
    "    noise_shape = [num_nonzero_elems]\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += tf.random_uniform(noise_shape)\n",
    "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "    pre_out = tf.sparse_retain(x, dropout_mask)\n",
    "    return pre_out * (1./keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geo-Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GeoVec():\n",
    "    def __init__(self, corpus=None, vocab_size=10, h_layers = [8, 4], \n",
    "                 act = tf.nn.relu, dropout=0.0, learning_rate = 1e-3):\n",
    "        \"\"\"Geo-Vec model as described in the report model section.\"\"\"\n",
    "        \n",
    "        self.corpus = corpus\n",
    "        self.vocab_size = vocab_size\n",
    "        self.h_layers = h_layers\n",
    "        self.act = act\n",
    "        self.dropout = dropout\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # use for plotting\n",
    "        self._loss_vals, self._acc_vals = [], []\n",
    "        \n",
    "        #placeholders\n",
    "        s = [self.vocab_size, self.vocab_size]\n",
    "        self.placeholders = {\n",
    "            'A_o': tf.sparse_placeholder(tf.float32),\n",
    "            'L_o': tf.sparse_placeholder(tf.float32),\n",
    "            'A_i': tf.sparse_placeholder(tf.float32),\n",
    "            'L_i': tf.sparse_placeholder(tf.float32),\n",
    "            'dropout': tf.placeholder_with_default(0., shape=())\n",
    "        }\n",
    "        \n",
    "        # model\n",
    "        self.aux_losses = None\n",
    "        dummy = sp2tf(ss.eye(self.vocab_size))\n",
    "        self.init_model(x=dummy)\n",
    "\n",
    "        #optimizer\n",
    "        self.init_optimizer()\n",
    "        \n",
    "        #sess\n",
    "        self.trained = 0\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def init_model(self, x, aux_tasks = None):\n",
    "        \"\"\"geo-vec model with variable number of gcn layers. Optional aux_taks\n",
    "        param is now unimplemented to specify which tasks to add. All aux losses\n",
    "        should be gathered in a self.aux_losses variable to gather later on.\"\"\"\n",
    "        for i, h_layer in enumerate(self.h_layers):\n",
    "            if i == 0:\n",
    "                h = self.gcn(x, self.vocab_size, self.h_layers[0], self.act, layer=i,sparse=True)  \n",
    "            elif (i+1) < len(self.h_layers):\n",
    "                h = self.gcn(h, self.h_layers[i-1], h_layer, self.act, layer=i, )\n",
    "            else:\n",
    "                self.emb_o, self.emb_i = self.gcn(h, self.h_layers[i-1], \n",
    "                                             h_layer, act=lambda x: x, layer=i,separate=True)\n",
    "        \n",
    "        # here we can left multiply the last layer h\n",
    "        # and perform auxilliary tasks.\n",
    "        \n",
    "        self.recon_o = self.decode(self.emb_o)\n",
    "        self.recon_i = self.decode(self.emb_i)\n",
    "    \n",
    "    def gcn(self, x, dim_in, dim_out, act, layer, sparse=False, separate=False):\n",
    "        \"\"\"basic graph convolution using a split up adjacency matrix.\n",
    "        The separation param is to create the final embeddings to reconstruct.\"\"\"\n",
    "        w1 = tf.get_variable('w1_{}'.format(layer), shape=[dim_in, dim_out], \n",
    "                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "        w2 = tf.get_variable('w2_{}'.format(layer), shape=[dim_in, dim_out], \n",
    "                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        if sparse:\n",
    "            x1 = tf.sparse_tensor_dense_matmul(x, w1)\n",
    "            x2 = tf.sparse_tensor_dense_matmul(x, w2)\n",
    "        else:\n",
    "            x1 = tf.matmul(x, w1)\n",
    "            x2 = tf.matmul(x, w2)\n",
    "            \n",
    "        x1 = tf.sparse_tensor_dense_matmul(self.placeholders['L_o'], x1)\n",
    "        x2 = tf.sparse_tensor_dense_matmul(self.placeholders['L_i'], x2)\n",
    "        \n",
    "        if separate:\n",
    "            return self.act(x1), self.act(x2)\n",
    "        \n",
    "        return self.act(x1 + x2)\n",
    "    \n",
    "    def decode(self, x, cap = 1000):\n",
    "        \"\"\"simple innerproduct decoder with sigmoid activation to scale\n",
    "        the edged between 0-1000 (assuming more co-occurances are unlikely).\"\"\"\n",
    "        x = tf.nn.dropout(x, 1-self.dropout)\n",
    "        x = tf.reshape(tf.matmul(x, tf.transpose(x)), [-1])\n",
    "        return tf.sigmoid(x)*cap\n",
    "        \n",
    "    def init_optimizer(self):\n",
    "        \"\"\"initializes optimizer and computes loss + accuracy. The loss function\n",
    "        is currently a MSE, due to the fact we are dealing with weighted edges.\n",
    "        This does not seem ideal, and should be thought about.\"\"\"\n",
    "        \n",
    "        labels_o = tf.reshape(tf.sparse_tensor_to_dense(\n",
    "                                self.placeholders['A_o'],validate_indices=False), [-1])\n",
    "        labels_i = tf.reshape(tf.sparse_tensor_to_dense(\n",
    "                                self.placeholders['A_i'],validate_indices=False), [-1])\n",
    "        loss_o = tf.losses.mean_squared_error(self.recon_o, labels_o)\n",
    "        loss_i = tf.losses.mean_squared_error(self.recon_i, labels_i) \n",
    "        self.loss = loss_o + loss_i\n",
    "        \n",
    "        # gather aux losses and add to total loss\n",
    "        if self.aux_losses:\n",
    "            self.loss += self.aux_losses\n",
    "        \n",
    "        # optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        self.opt_op = optimizer.minimize(self.loss)\n",
    "\n",
    "        cp_o = tf.equal(tf.cast(tf.greater_equal(self.recon_o, 0.5), tf.int32),\n",
    "                                                   tf.cast(labels_o, tf.int32))\n",
    "        cp_i = tf.equal(tf.cast(tf.greater_equal(self.recon_i, 0.5), tf.int32),\n",
    "                                                   tf.cast(labels_i, tf.int32))\n",
    "        correct_prediction = tf.concat([cp_o, cp_i], 0)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    def get_feed_dict(self, A_o, A_i, L_o, L_i):\n",
    "        feed_dict = {self.placeholders['A_o']: A_o,\n",
    "                     self.placeholders['A_i']: A_i,\n",
    "                     self.placeholders['L_o']: L_o,\n",
    "                     self.placeholders['L_i']: L_i}\n",
    "        return feed_dict\n",
    "    \n",
    "    def get_sample(self):\n",
    "        \"\"\"get random sample from corpus graph cache\"\"\"\n",
    "        dummy = [] \n",
    "        for i in range(4):\n",
    "            dummy.append(sp2tf(ss.eye(self.vocab_size)))\n",
    "\n",
    "        return dummy\n",
    "    \n",
    "    def train(self, num_epochs = 100, print_freq=50):\n",
    "        \"\"\"train op that can be invoked multiple times.\"\"\"\n",
    "        tf.set_random_seed(42)\n",
    "        np.random.seed(42)\n",
    "\n",
    "        for e in range(num_epochs):\n",
    "            self.trained += 1\n",
    "            A_o, A_i, L_o, L_i = self.get_sample()\n",
    "            feed_dict = self.get_feed_dict(A_o, A_i, L_o, L_i)\n",
    "    \n",
    "            outs = self.sess.run([self.opt_op, self.loss, self.accuracy], feed_dict=feed_dict)\n",
    "            avg_loss, avg_acc = outs[1], outs[2]\n",
    "            self._loss_vals.append(avg_loss)\n",
    "            self._acc_vals.append(avg_acc)\n",
    "            \n",
    "            print('\\r epoch: %d/%d \\t loss: %.3f \\t avg_acc: %.3f' \n",
    "                      % (e+1, num_epochs, avg_loss, avg_acc), end='')\n",
    "            if (e + 1) % print_freq == 0:\n",
    "                print('')\n",
    "        else:\n",
    "            print('----> done training: {} epochs'.format(self.trained))\n",
    "        \n",
    "    def plot(self):\n",
    "        \"\"\"Plotting loss function\"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(self._loss_vals, color='red')\n",
    "        plt.plot(self._acc_vals, color='blue')\n",
    "        \n",
    "        plt.legend(handles=[mpatches.Patch(color='red', label='loss'),\n",
    "                            mpatches.Patch(color='blue', label='acc')],\n",
    "                   bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "        plt.show()\n",
    "        \n",
    "    def get_reconstruction(self, doc = None):\n",
    "        A_o, A_i, L_o, L_i = self.get_sample()\n",
    "        feed_dict = self.get_feed_dict(A_o, A_i, L_o, L_i)\n",
    "        recon_o, recon_i = self.sess.run([self.recon_o, self.recon_i], feed_dict=feed_dict)\n",
    "        return A_o, A_i, recon_o, recon_i\n",
    "    \n",
    "    def get_embeddings(self, doc = None):\n",
    "        A_o, A_i, L_o, L_i = self.get_sample()\n",
    "        feed_dict = self.get_feed_dict(A_o, A_i, L_o, L_i)\n",
    "        emb_o, emb_i = self.sess.run([self.em_o, self.emb_i], feed_dict=feed_dict)\n",
    "        return A_o, A_i, emb_o, emb_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch: 100/1000 \t loss: 499820.750 \t avg_acc: 0.001\n",
      " epoch: 200/1000 \t loss: 499816.844 \t avg_acc: 0.001\n",
      " epoch: 300/1000 \t loss: 499816.375 \t avg_acc: 0.001\n",
      " epoch: 400/1000 \t loss: 499816.188 \t avg_acc: 0.001\n",
      " epoch: 500/1000 \t loss: 499816.125 \t avg_acc: 0.001\n",
      " epoch: 600/1000 \t loss: 499816.062 \t avg_acc: 0.001\n",
      " epoch: 700/1000 \t loss: 499816.062 \t avg_acc: 0.001\n",
      " epoch: 800/1000 \t loss: 499816.062 \t avg_acc: 0.001\n",
      " epoch: 900/1000 \t loss: 499816.062 \t avg_acc: 0.001\n",
      " epoch: 1000/1000 \t loss: 499816.062 \t avg_acc: 0.001\n",
      "----> done training: 1000 epochs\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "geo_vec_model = GeoVec(vocab_size=1000, h_layers = [32, 16])\n",
    "geo_vec_model.train(1000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzEAAAFpCAYAAACs1KYzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noZrS6AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12265fef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "geo_vec_model.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch3]",
   "language": "python",
   "name": "conda-env-pytorch3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
