{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reuters dataset Loading and Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load docs\n",
    "docs = []\n",
    "topic_str = []\n",
    "for datafile in sorted(listdir('data/reuters/')):\n",
    "    with open('data/reuters/{}'.format(datafile), 'rb') as f:\n",
    "        if datafile.endswith('.sgm'):\n",
    "            print('Loading {}...'.format(datafile))\n",
    "            soup = BeautifulSoup(f.read(), 'lxml')\n",
    "            for node in soup.findAll('text'):\n",
    "                docs.append(''.join(node.findAll(text=True)))\n",
    "            for node in soup.findAll('topics'):\n",
    "                topic_str.append(node.findAll(text=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocess topics\n",
    "with open('data/reuters/all-topics-strings.lc.txt') as f:\n",
    "    topiclist = [x.strip() for x in f.readlines()]\n",
    "topic2id = dict(zip(topiclist, np.arange(len(topiclist))))\n",
    "id2topic = {v: k for k, v in topic2id.items()}\n",
    "topics = [[topic2id[x] for x in y] for y in topic_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocess documents\n",
    "splitted_docs = [re.sub('[^a-zA-Z]+', ' ', doc) for doc in docs]\n",
    "splitted_docs = [doc.split(' ') for doc in splitted_docs]\n",
    "splitted_docs = [[word for word in doc if word != ''] for doc in splitted_docs]\n",
    "words = [x for y in splitted_docs for x in y]\n",
    "unique_words, unique_words_c = np.unique(words, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, unique_words_sort = zip(*sorted(zip(unique_words_c, unique_words), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "word2id = dict(zip(unique_words_sort, np.arange(len(unique_words_sort))))\n",
    "id2word = {v: k for k, v in word2id.items()}\n",
    "tokenized = [[word2id[word] if word in word2id else -1 for word in doc] for doc in splitted_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save tokenized reuters\n",
    "np.save('data/reuters/reuters_topics.npy', topics)\n",
    "np.save('data/reuters/reuters_topic2id.npy', topic2id)\n",
    "np.save('data/reuters/reuters_id2topic.npy', id2topic)\n",
    "np.save('data/reuters/reuters_word2id.npy', word2id)\n",
    "np.save('data/reuters/reuters_id2word.npy', id2word)\n",
    "np.save('data/reuters/reuters_tokenized.npy', tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load tokenized reuters\n",
    "topic2id = np.load('data/reuters/reuters_topic2id.npy').item(0)\n",
    "id2topic = np.load('data/reuters/reuters_id2topic.npy').item(0)\n",
    "topics = list(np.load('data/reuters/reuters_topics.npy'))\n",
    "\n",
    "word2id = np.load('data/reuters/reuters_word2id.npy').item(0)\n",
    "id2word = np.load('data/reuters/reuters_id2word.npy').item(0)\n",
    "tokenized = list(np.load('data/reuters/reuters_tokenized.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example document\n",
    "tokenized_doc = tokenized[0]\n",
    "regular_doc = ' '.join([id2word[x] for x in tokenized_doc])\n",
    "tokenized_topic = topics[0]\n",
    "regular_topic = ' '.join([id2topic[x] for x in tokenized_topic])\n",
    "\n",
    "print('[TOPIC]\\n', regular_topic)\n",
    "print('\\n[DOC]\\n', regular_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigdocs = '\\n'.join([' '.join([id2word[word_id] for word_id in doc]) for doc in tokenized])\n",
    "with open('data/reuters/raw.txt', 'w') as f:\n",
    "    f.write(bigdocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
