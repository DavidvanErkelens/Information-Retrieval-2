{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geo-Vec Model \n",
    "- basic geo-vec model\n",
    "- auxilliary task models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as ss\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Doc2Graph():\n",
    "    \"\"\"Convert tokenized document to weighted Adjacency matrix\n",
    "    and graph Laplacian\"\"\"\n",
    "    def __init__(self, doc, doc_id=-1):\n",
    "        self.doc = doc\n",
    "        self.doc_id = doc_id\n",
    "        \n",
    "    def doc2graph(self):\n",
    "        g = self.load()\n",
    "            \n",
    "        if not g:\n",
    "            As = self.get_As()\n",
    "            Ls = self.get_Ls(As)\n",
    "            g = As + Ls\n",
    "            self.save(g)\n",
    "\n",
    "        return g\n",
    "    \n",
    "    def get_As(self):\n",
    "        \"\"\"Get the weighted adjacency matrices of incoming\n",
    "        and outcoming edges\"\"\"\n",
    "        As = []\n",
    "        e = np.vstack((np.reshape(self.doc, (-1, 2)),\n",
    "                       np.reshape(self.doc[1:-1], (-1, 2))))  \n",
    "        e1 = np.flip(e, 1)\n",
    "        e1 = e1[np.lexsort((e1[:,1], e1[:,0]))]\n",
    "        e2 = e[np.lexsort((e[:,1], e[:,0]))]\n",
    "        for a in [e2, e1]:\n",
    "            rc, cooc = np.unique(e1, axis=0, return_counts=True)\n",
    "            As.append(ss.coo_matrix((cooc, (rc[:,0], rc[:,1])), \n",
    "                                   tuple((np.max(a)+1, np.max(a)+1))))\n",
    "        return As\n",
    "    \n",
    "    def get_Ls(self, As, renorm_trick=True):\n",
    "        \"\"\"Create graph Laplacians from adjacency matrices\"\"\"\n",
    "        Ls = []\n",
    "        for A in As:\n",
    "            A = ss.coo_matrix(A)\n",
    "            if renorm_trick:\n",
    "                A += ss.eye(A.shape[0])\n",
    "            D_inv_sqrt = ss.diags(np.power(np.array(A.sum(1)), -0.5).flatten())\n",
    "            L = A.dot(D_inv_sqrt).transpose().dot(D_inv_sqrt).tocoo()\n",
    "            Ls.append(L)\n",
    "            \n",
    "        return Ls\n",
    "    \n",
    "    def save(self, g):\n",
    "        \"\"\"Save graph to folder for reuse\"\"\"\n",
    "        print('save: implement me!')\n",
    "        pass\n",
    "    \n",
    "    def load(self):\n",
    "        print('load: implement me!')\n",
    "        return None\n",
    "            \n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    if not ss.isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape\n",
    "\n",
    "def sp2tf(sp_t):\n",
    "    t = sparse_to_tuple(sp_t)\n",
    "#     tensor = tf.SparseTensor(t[0],t[1].astype(np.float32),t[2])\n",
    "    tensor = tf.SparseTensorValue(t[0],t[1].astype(np.float32),t[2])\n",
    "    return tensor\n",
    "\n",
    "def dropout_sparse(x, keep_prob, num_nonzero_elems):\n",
    "    \"\"\"Dropout for sparse tensors. Currently fails for very large sparse tensors (>1M elements)\n",
    "    \"\"\"\n",
    "    noise_shape = [num_nonzero_elems]\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += tf.random_uniform(noise_shape)\n",
    "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "    pre_out = tf.sparse_retain(x, dropout_mask)\n",
    "    return pre_out * (1./keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geo-Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GeoVec():\n",
    "    def __init__(self, corpus=None, vocab_size=10, h_layers = [8, 4], \n",
    "                 act = tf.nn.relu, dropout=0.0, learning_rate = 1e-3):\n",
    "        \"\"\"Geo-Vec model as described in the report model section.\"\"\"\n",
    "        \n",
    "        self.corpus = corpus\n",
    "        self.vocab_size = vocab_size\n",
    "        self.h_layers = h_layers\n",
    "        self.act = act\n",
    "        self.dropout = dropout\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # use for plotting\n",
    "        self._loss_vals, self._acc_vals = [], []\n",
    "        \n",
    "        #placeholders\n",
    "        s = [self.vocab_size, self.vocab_size]\n",
    "        self.placeholders = {\n",
    "            'A_o': tf.sparse_placeholder(tf.float32),\n",
    "            'L_o': tf.sparse_placeholder(tf.float32),\n",
    "            'A_i': tf.sparse_placeholder(tf.float32),\n",
    "            'L_i': tf.sparse_placeholder(tf.float32),\n",
    "            'dropout': tf.placeholder_with_default(0., shape=())\n",
    "        }\n",
    "        \n",
    "        # model\n",
    "        self.aux_losses = None\n",
    "        dummy = sp2tf(ss.eye(self.vocab_size))\n",
    "        self.init_model(x=dummy)\n",
    "\n",
    "        #optimizer\n",
    "        self.init_optimizer()\n",
    "        \n",
    "        #sess\n",
    "        self.trained = 0\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def init_model(self, x, aux_tasks = None):\n",
    "        \"\"\"geo-vec model with variable number of gcn layers. Optional aux_taks\n",
    "        param is now unimplemented to specify which tasks to add. All aux losses\n",
    "        should be gathered in a self.aux_losses variable to gather later on.\"\"\"\n",
    "        for i, h_layer in enumerate(self.h_layers):\n",
    "            if i == 0:\n",
    "                h = self.gcn(x, self.vocab_size, self.h_layers[0], self.act, layer=i,sparse=True)  \n",
    "            elif (i+1) < len(self.h_layers):\n",
    "                h = self.gcn(h, self.h_layers[i-1], h_layer, self.act, layer=i, )\n",
    "            else:\n",
    "                self.emb_o, self.emb_i = self.gcn(h, self.h_layers[i-1], \n",
    "                                             h_layer, act=lambda x: x, layer=i,separate=True)\n",
    "        \n",
    "        # here we can left multiply the last layer h\n",
    "        # and perform auxilliary tasks.\n",
    "        \n",
    "        self.recon_o = self.decode(self.emb_o)\n",
    "        self.recon_i = self.decode(self.emb_i)\n",
    "    \n",
    "    def gcn(self, x, dim_in, dim_out, act, layer, sparse=False, separate=False):\n",
    "        \"\"\"basic graph convolution using a split up adjacency matrix.\n",
    "        The separation param is to create the final embeddings to reconstruct.\"\"\"\n",
    "        w1 = tf.get_variable('w1_{}'.format(layer), shape=[dim_in, dim_out], \n",
    "                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "        w2 = tf.get_variable('w2_{}'.format(layer), shape=[dim_in, dim_out], \n",
    "                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        if sparse:\n",
    "            x1 = tf.sparse_tensor_dense_matmul(x, w1)\n",
    "            x2 = tf.sparse_tensor_dense_matmul(x, w2)\n",
    "        else:\n",
    "            x1 = tf.matmul(x, w1)\n",
    "            x2 = tf.matmul(x, w2)\n",
    "            \n",
    "        x1 = tf.sparse_tensor_dense_matmul(self.placeholders['L_o'], x1)\n",
    "        x2 = tf.sparse_tensor_dense_matmul(self.placeholders['L_i'], x2)\n",
    "        \n",
    "        if separate:\n",
    "            return self.act(x1), self.act(x2)\n",
    "        \n",
    "        return self.act(x1 + x2)\n",
    "    \n",
    "    def decode(self, x, cap = 1000):\n",
    "        \"\"\"simple innerproduct decoder with sigmoid activation to scale\n",
    "        the edged between 0-1000 (assuming more co-occurances are unlikely).\"\"\"\n",
    "        x = tf.nn.dropout(x, 1-self.dropout)\n",
    "        x = tf.reshape(tf.matmul(x, tf.transpose(x)), [-1])\n",
    "        return tf.sigmoid(x)*cap\n",
    "        \n",
    "    def init_optimizer(self):\n",
    "        \"\"\"initializes optimizer and computes loss + accuracy. The loss function\n",
    "        is currently a MSE, due to the fact we are dealing with weighted edges.\n",
    "        This does not seem ideal, and should be thought about.\"\"\"\n",
    "        labels_o = tf.reshape(tf.sparse_tensor_to_dense(\n",
    "                                self.placeholders['A_o'],validate_indices=False), [-1])\n",
    "        labels_i = tf.reshape(tf.sparse_tensor_to_dense(\n",
    "                                self.placeholders['A_i'],validate_indices=False), [-1])\n",
    "        loss_o = tf.losses.mean_squared_error(self.recon_o, labels_o)\n",
    "        loss_i = tf.losses.mean_squared_error(self.recon_i, labels_i) \n",
    "        self.loss = loss_o + loss_i\n",
    "        \n",
    "        # gather aux losses and add to total loss\n",
    "        if self.aux_losses:\n",
    "            self.loss += self.aux_losses\n",
    "        \n",
    "        # optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        self.opt_op = optimizer.minimize(self.loss)\n",
    "\n",
    "        cp_o = tf.equal(tf.cast(tf.greater_equal(self.recon_o, 0.5), tf.int32),\n",
    "                                                   tf.cast(labels_o, tf.int32))\n",
    "        cp_i = tf.equal(tf.cast(tf.greater_equal(self.recon_i, 0.5), tf.int32),\n",
    "                                                   tf.cast(labels_i, tf.int32))\n",
    "        correct_prediction = tf.concat([cp_o, cp_i], 0)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    def get_feed_dict(self, A_o, A_i, L_o, L_i):\n",
    "        feed_dict = {self.placeholders['A_o']: A_o,\n",
    "                     self.placeholders['A_i']: A_i,\n",
    "                     self.placeholders['L_o']: L_o,\n",
    "                     self.placeholders['L_i']: L_i}\n",
    "        return feed_dict\n",
    "    \n",
    "    def get_sample(self):\n",
    "        \"\"\"get random sample from corpus graph cache\"\"\"\n",
    "        dummy = Doc2Graph(fake_doc).doc2graph()\n",
    "        print (len(dummy))\n",
    "        for i, d in enumerate(dummy):\n",
    "            dummy[i] = sp2tf(d)\n",
    "\n",
    "        return dummy\n",
    "    \n",
    "    def train(self, num_epochs = 100, print_freq=50):\n",
    "        \"\"\"train op that can be invoked multiple times.\"\"\"\n",
    "        tf.set_random_seed(42)\n",
    "        np.random.seed(42)\n",
    "\n",
    "        for e in range(num_epochs):\n",
    "            self.trained += 1\n",
    "            A_o, A_i, L_o, L_i = self.get_sample()\n",
    "            feed_dict = self.get_feed_dict(A_o, A_i, L_o, L_i)\n",
    "    \n",
    "            outs = self.sess.run([self.opt_op, self.loss, self.accuracy], feed_dict=feed_dict)\n",
    "            avg_loss, avg_acc = outs[1], outs[2]\n",
    "            self._loss_vals.append(avg_loss)\n",
    "            self._acc_vals.append(avg_acc)\n",
    "            \n",
    "            print('\\r epoch: %d/%d \\t loss: %.3f \\t avg_acc: %.3f' \n",
    "                      % (e+1, num_epochs, avg_loss, avg_acc), end='')\n",
    "            if (e + 1) % print_freq == 0:\n",
    "                print('')\n",
    "        else:\n",
    "            print('----> done training: {} epochs'.format(self.trained))\n",
    "        \n",
    "    def plot(self):\n",
    "        \"\"\"Plotting loss function\"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(self._loss_vals, color='red')\n",
    "        plt.plot(self._acc_vals, color='blue')\n",
    "        \n",
    "        plt.legend(handles=[mpatches.Patch(color='red', label='loss'),\n",
    "                            mpatches.Patch(color='blue', label='acc')],\n",
    "                   bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "        plt.show()\n",
    "        \n",
    "    def get_reconstruction(self, doc = None):\n",
    "        if doc:\n",
    "            A_o, A_i, L_o, L_i = Doc2Graph(doc, doc_id).doc2graph()\n",
    "        else:\n",
    "            A_o, A_i, L_o, L_i = self.get_sample()\n",
    "            \n",
    "        feed_dict = self.get_feed_dict(A_o, A_i, L_o, L_i)\n",
    "        recon_o, recon_i = self.sess.run([self.recon_o, self.recon_i], feed_dict=feed_dict)\n",
    "        return A_o, A_i, recon_o, recon_i\n",
    "    \n",
    "    def get_embeddings(self, doc = None, doc_id = None):\n",
    "        if doc:\n",
    "            A_o, A_i, L_o, L_i = Doc2Graph(doc, doc_id).doc2graph()\n",
    "        else:\n",
    "            A_o, A_i, L_o, L_i = self.get_sample()\n",
    "            \n",
    "        feed_dict = self.get_feed_dict(A_o, A_i, L_o, L_i)\n",
    "        emb_o, emb_i = self.sess.run([self.em_o, self.emb_i], feed_dict=feed_dict)\n",
    "        return A_o, A_i, emb_o, emb_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "fake_doc = np.asarray([1, 2, 3, 4, 1, 5, 6, 4, 1, 7, 9])\n",
    "geo_vec_model = GeoVec(vocab_size=10, h_layers = [6, 4])\n",
    "geo_vec_model.train(1000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "geo_vec_model.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch3]",
   "language": "python",
   "name": "conda-env-pytorch3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
