{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "import collections\n",
    "from itertools import compress\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "# Set random seeds\n",
    "SEED = 2016\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(words, vocabulary_size=50000):\n",
    "    '''\n",
    "    Build the dictionary and replace rare words with UNK token.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    words: list of tokens\n",
    "    vocabulary_size: maximum number of top occurring tokens to produce,\n",
    "        rare tokens will be replaced by 'UNK'\n",
    "    '''\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "    dictionary = dict() # {word: index}\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "        data = list() # collect index\n",
    "        unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count # list of tuples (word, count)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    print(len(data), data[0])\n",
    "    print(len(count), count[0])\n",
    "    print(len(dictionary), dictionary[0])\n",
    "    print(len(reverse_dictionary), reverse_dictionary[0])\n",
    "    # print(count)\n",
    "    # print(dictionary)\n",
    "    # print(reverse_dictionary)\n",
    "    return data, count, dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_doc_dataset(docs, vocabulary_size=50000):\n",
    "    '''\n",
    "    Build the dictionary and replace rare words with UNK token.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    docs: list of token lists, each token list represent a sentence/document\n",
    "    vocabulary_size: maximum number of top occurring tokens to produce, \n",
    "        rare tokens will be replaced by 'UNK'\n",
    "    '''\n",
    "    count = [['UNK', -1]]\n",
    "    # words = reduce(lambda x,y: x+y, docs)\n",
    "    words = []\n",
    "    doc_ids = [] # collect document(sentence) indices\n",
    "    for i, doc in enumerate(docs):\n",
    "        doc_ids.extend([i] * len(doc))\n",
    "        words.extend(doc)\n",
    "\n",
    "    word_ids, count, dictionary, reverse_dictionary = build_dataset(words, vocabulary_size=vocabulary_size)\n",
    "\n",
    "    return doc_ids, word_ids, count, dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "\n",
    "def generate_batch_pvdm(doc_ids, word_ids, batch_size, window_size):\n",
    "    '''\n",
    "    Batch generator for PV-DM (Distributed Memory Model of Paragraph Vectors).\n",
    "    batch should be a shape of (batch_size, window_size+1)\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc_ids: list of document indices \n",
    "    word_ids: list of word indices\n",
    "    batch_size: number of words in each mini-batch\n",
    "    window_size: number of leading words before the target word \n",
    "    '''\n",
    "    global data_index\n",
    "    assert batch_size % window_size == 0\n",
    "    batch = np.ndarray(shape=(batch_size, window_size + 1), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = window_size + 1\n",
    "    buffer = collections.deque(maxlen=span) # used for collecting word_ids[data_index] in the sliding window\n",
    "    buffer_doc = collections.deque(maxlen=span) # collecting id of documents in the sliding window\n",
    "    # collect the first window of words\n",
    "    for _ in range(span):\n",
    "        buffer.append(word_ids[data_index])\n",
    "        buffer_doc.append(doc_ids[data_index])\n",
    "        data_index = (data_index + 1) % len(word_ids)\n",
    "\n",
    "    mask = [1] * span\n",
    "    mask[-1] = 0 \n",
    "    i = 0\n",
    "    while i < batch_size:\n",
    "        if len(set(buffer_doc)) == 1:\n",
    "            doc_id = buffer_doc[-1]\n",
    "            # all leading words and the doc_id\n",
    "            batch[i, :] = list(compress(buffer, mask)) + [doc_id]\n",
    "            labels[i, 0] = buffer[-1] # the last word at end of the sliding window\n",
    "            i += 1\n",
    "        # move the sliding window  \n",
    "        buffer.append(word_ids[data_index])\n",
    "        buffer_doc.append(doc_ids[data_index])\n",
    "        data_index = (data_index + 1) % len(word_ids)\n",
    "\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc2Vec(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, batch_size=128, window_size=8, \n",
    "        concat=True,\n",
    "        architecture='pvdm', embedding_size_w=128, \n",
    "        embedding_size_d=128,\n",
    "        vocabulary_size=50000, \n",
    "        document_size=10000,\n",
    "        loss_type='nce_loss', n_neg_samples=64,\n",
    "        optimize='Adagrad', \n",
    "        learning_rate=1.0, n_steps=100001):\n",
    "        # bind params to class\n",
    "        self.batch_size = batch_size\n",
    "        self.window_size = window_size\n",
    "        self.concat = concat\n",
    "        self.architecture = architecture\n",
    "        self.embedding_size_w = embedding_size_w\n",
    "        self.embedding_size_d = embedding_size_d\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.document_size = document_size\n",
    "        self.loss_type = loss_type\n",
    "        self.n_neg_samples = n_neg_samples \n",
    "        self.optimize = optimize\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_steps = n_steps\n",
    "\n",
    "        # choose a batch_generator function for feed_dict\n",
    "        self._choose_batch_generator()\n",
    "        # init all variables in a tensorflow graph\n",
    "        self._init_graph()\n",
    "\n",
    "        # create a session\n",
    "        self.sess = tf.Session(graph=self.graph)\n",
    "\n",
    "    def _choose_batch_generator(self):\n",
    "        if self.architecture == 'pvdm':\n",
    "            self.generate_batch = generate_batch_pvdm\n",
    "\n",
    "    def _init_graph(self):\n",
    "        '''\n",
    "        Init a tensorflow Graph containing:\n",
    "        input data, variables, model, loss function, optimizer\n",
    "        '''\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default(), tf.device('/cpu:0'):\n",
    "            # Set graph level random seed\n",
    "            tf.set_random_seed(SEED)\n",
    "\n",
    "            self.train_dataset = tf.placeholder(tf.int32, shape=[self.batch_size, self.window_size+1])\n",
    "            self.train_labels = tf.placeholder(tf.int32, shape=[self.batch_size, 1])\n",
    "            # Variables.\n",
    "            # embeddings for words, W in paper\n",
    "            self.word_embeddings = tf.Variable(\n",
    "                tf.random_uniform([self.vocabulary_size, self.embedding_size_w], -1.0, 1.0))\n",
    "\n",
    "            # embedding for documents (can be sentences or paragraph), D in paper\n",
    "            self.doc_embeddings = tf.Variable(\n",
    "                tf.random_uniform([self.document_size, self.embedding_size_d], -1.0, 1.0))\n",
    "\n",
    "            if self.concat: # concatenating word vectors and doc vector\n",
    "                combined_embed_vector_length = self.embedding_size_w * self.window_size + self.embedding_size_d\n",
    "            else: # concatenating the average of word vectors and the doc vector \n",
    "                combined_embed_vector_length = self.embedding_size_w + self.embedding_size_d\n",
    "\n",
    "            # softmax weights, W and D vectors should be concatenated before applying softmax\n",
    "            self.weights = tf.Variable(\n",
    "                tf.truncated_normal([self.vocabulary_size, combined_embed_vector_length],\n",
    "                    stddev=1.0 / math.sqrt(combined_embed_vector_length)))\n",
    "            # softmax biases\n",
    "            self.biases = tf.Variable(tf.zeros([self.vocabulary_size]))\n",
    "\n",
    "            # Model.\n",
    "            # Look up embeddings for inputs.\n",
    "            # shape: (batch_size, embeddings_size)\n",
    "            embed = [] # collect embedding matrices with shape=(batch_size, embedding_size)\n",
    "            if self.concat:\n",
    "                for j in range(self.window_size):\n",
    "                    embed_w = tf.nn.embedding_lookup(self.word_embeddings, self.train_dataset[:, j])\n",
    "                    embed.append(embed_w)\n",
    "            else:\n",
    "                # averaging word vectors\n",
    "                embed_w = tf.zeros([self.batch_size, self.embedding_size_w])\n",
    "                for j in range(self.window_size):\n",
    "                    embed_w += tf.nn.embedding_lookup(self.word_embeddings, self.train_dataset[:, j])\n",
    "                embed.append(embed_w)\n",
    "                    \n",
    "            embed_d = tf.nn.embedding_lookup(self.doc_embeddings, self.train_dataset[:, self.window_size])\n",
    "            embed.append(embed_d)\n",
    "            # concat word and doc vectors\n",
    "            self.embed = tf.concat(embed, 1)\n",
    "\n",
    "            # Compute the loss, using a sample of the negative labels each time.\n",
    "            if self.loss_type == 'sampled_softmax_loss':\n",
    "                loss = tf.nn.sampled_softmax_loss(self.weights, self.biases, self.train_labels,\n",
    "                    self.embed, self.n_neg_samples, self.vocabulary_size)\n",
    "            elif self.loss_type == 'nce_loss':\n",
    "                loss= tf.nn.nce_loss(self.weights, self.biases, self.train_labels, \n",
    "                    self.embed, self.n_neg_samples, self.vocabulary_size)\n",
    "            self.loss = tf.reduce_mean(loss)\n",
    "\n",
    "            # Optimizer.\n",
    "            if self.optimize == 'Adagrad':\n",
    "                self.optimizer = tf.train.AdagradOptimizer(self.learning_rate).minimize(loss)\n",
    "            elif self.optimize == 'SGD':\n",
    "                self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(loss)\n",
    "\n",
    "            # Compute the similarity between minibatch examples and all embeddings.\n",
    "            # We use the cosine distance:\n",
    "            norm_w = tf.sqrt(tf.reduce_sum(tf.square(self.word_embeddings), 1, keep_dims=True))\n",
    "            self.normalized_word_embeddings = self.word_embeddings / norm_w\n",
    "\n",
    "            norm_d = tf.sqrt(tf.reduce_sum(tf.square(self.doc_embeddings), 1, keep_dims=True))\n",
    "            self.normalized_doc_embeddings = self.doc_embeddings / norm_d\n",
    "\n",
    "            # init op \n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "            # create a saver \n",
    "            self.saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "    def _build_dictionaries(self, docs):\n",
    "        '''\n",
    "        Process tokens and build dictionaries mapping between tokens and \n",
    "        their indices. Also generate token count and bind these to self.\n",
    "        '''\n",
    "\n",
    "        doc_ids, word_ids, count, dictionary, reverse_dictionary = build_doc_dataset(docs, \n",
    "            self.vocabulary_size)\n",
    "        self.dictionary = dictionary\n",
    "        self.reverse_dictionary = reverse_dictionary\n",
    "        self.count = count\n",
    "        return doc_ids, word_ids\n",
    "\n",
    "\n",
    "    def fit(self, docs):\n",
    "        '''\n",
    "        words: a list of words. \n",
    "        '''\n",
    "        # pre-process words to generate indices and dictionaries\n",
    "        doc_ids, word_ids = self._build_dictionaries(docs)\n",
    "\n",
    "        # with self.sess as session:\n",
    "        session = self.sess\n",
    "\n",
    "        session.run(self.init_op)\n",
    "\n",
    "        average_loss = 0\n",
    "        print(\"Initialized\")\n",
    "        for step in range(self.n_steps):\n",
    "            batch_data, batch_labels = self.generate_batch(doc_ids, word_ids,\n",
    "                self.batch_size, self.window_size)\n",
    "            feed_dict = {self.train_dataset : batch_data, self.train_labels : batch_labels}\n",
    "            \n",
    "            op, l = session.run([self.optimizer, self.loss], feed_dict=feed_dict)\n",
    "            average_loss += l\n",
    "            if step % 2000 == 0:\n",
    "                if step > 0:\n",
    "                    average_loss = average_loss / 2000\n",
    "                # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "                print('Average loss at step %d: %f' % (step, average_loss))\n",
    "                average_loss = 0\n",
    "\n",
    "        # bind embedding matrices to self\n",
    "        self.word_embeddings = session.run(self.normalized_word_embeddings)\n",
    "        self.doc_embeddings = session.run(self.normalized_doc_embeddings)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def save(self, path):\n",
    "        '''\n",
    "        To save trained model and its params.\n",
    "        '''\n",
    "        save_path = self.saver.save(self.sess, \n",
    "            os.path.join(path, 'model.ckpt'))\n",
    "        # save parameters of the model\n",
    "        params = self.get_params()\n",
    "        json.dump(params, \n",
    "            open(os.path.join(path, 'model_params.json'), 'wb'))\n",
    "        \n",
    "        # save dictionary, reverse_dictionary\n",
    "        json.dump(self.dictionary, \n",
    "            open(os.path.join(path, 'model_dict.json'), 'wb'), \n",
    "            ensure_ascii=False)\n",
    "        json.dump(self.reverse_dictionary, \n",
    "            open(os.path.join(path, 'model_rdict.json'), 'wb'), \n",
    "            ensure_ascii=False)\n",
    "\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "        return save_path\n",
    "\n",
    "    def _restore(self, path):\n",
    "        with self.graph.as_default():\n",
    "            self.saver.restore(self.sess, path)\n",
    "\n",
    "    @classmethod\n",
    "    def restore(cls, path):\n",
    "        '''\n",
    "        To restore a saved model.\n",
    "        '''\n",
    "        # load params of the model\n",
    "        path_dir = os.path.dirname(path)\n",
    "        params = json.load(open(os.path.join(path_dir, 'model_params.json'), 'rb'))\n",
    "        # init an instance of this class\n",
    "        estimator = Doc2Vec(**params)\n",
    "        estimator._restore(path)\n",
    "        # evaluate the Variable embeddings and bind to estimator\n",
    "        estimator.word_embeddings = estimator.sess.run(estimator.normalized_word_embeddings)\n",
    "        estimator.doc_embeddings = estimator.sess.run(estimator.normalized_doc_embeddings)\n",
    "        # bind dictionaries \n",
    "        estimator.dictionary = json.load(open(os.path.join(path_dir, 'model_dict.json'), 'rb'))\n",
    "        reverse_dictionary = json.load(open(os.path.join(path_dir, 'model_rdict.json'), 'rb'))\n",
    "        # convert indices loaded from json back to int since json does not allow int as keys\n",
    "        estimator.reverse_dictionary = {int(key):val for key, val in reverse_dictionary.items()}\n",
    "\n",
    "        return estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2id = np.load('data/reuters/reuters_word2id.npy').item(0)\n",
    "id2word = np.load('data/reuters/reuters_id2word.npy').item(0)\n",
    "tokenized = np.load('data/reuters/reuters_tokenized.npy')\n",
    "docs = [[id2word[word_id] for word_id in doc] for doc in tokenized]\n",
    "doc_dataset = build_doc_dataset(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1334\n"
     ]
    }
   ],
   "source": [
    "print(max(len(x) for x in docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec = Doc2Vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 262.751007\n",
      "Average loss at step 2000: 248.325735\n",
      "Average loss at step 4000: 127.464425\n",
      "Average loss at step 6000: 93.298942\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "indices[113] = 10000 is not in [0, 10000)\n\t [[Node: embedding_lookup_8 = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@Variable_1\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable_1/read, strided_slice_8)]]\n\nCaused by op 'embedding_lookup_8', defined at:\n  File \"/home/tycho/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/tycho/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-101-a96bcf655007>\", line 1, in <module>\n    doc2vec = Doc2Vec()\n  File \"<ipython-input-100-773017841930>\", line 30, in __init__\n    self._init_graph()\n  File \"<ipython-input-100-773017841930>\", line 87, in _init_graph\n    embed_d = tf.nn.embedding_lookup(self.doc_embeddings, self.train_dataset[:, self.window_size])\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py\", line 294, in embedding_lookup\n    transform_fn=None)\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py\", line 123, in _embedding_lookup_and_transform\n    result = _gather_and_clip(params[0], ids, max_norm, name=name)\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py\", line 57, in _gather_and_clip\n    embs = array_ops.gather(params, ids, name=name)\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 2409, in gather\n    validate_indices=validate_indices, name=name)\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1219, in gather\n    validate_indices=validate_indices, name=name)\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): indices[113] = 10000 is not in [0, 10000)\n\t [[Node: embedding_lookup_8 = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@Variable_1\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable_1/read, strided_slice_8)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[113] = 10000 is not in [0, 10000)\n\t [[Node: embedding_lookup_8 = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@Variable_1\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable_1/read, strided_slice_8)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-07bfff9efe80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdoc2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-100-773017841930>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_labels\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0maverage_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[113] = 10000 is not in [0, 10000)\n\t [[Node: embedding_lookup_8 = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@Variable_1\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable_1/read, strided_slice_8)]]\n\nCaused by op 'embedding_lookup_8', defined at:\n  File \"/home/tycho/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/tycho/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-101-a96bcf655007>\", line 1, in <module>\n    doc2vec = Doc2Vec()\n  File \"<ipython-input-100-773017841930>\", line 30, in __init__\n    self._init_graph()\n  File \"<ipython-input-100-773017841930>\", line 87, in _init_graph\n    embed_d = tf.nn.embedding_lookup(self.doc_embeddings, self.train_dataset[:, self.window_size])\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py\", line 294, in embedding_lookup\n    transform_fn=None)\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py\", line 123, in _embedding_lookup_and_transform\n    result = _gather_and_clip(params[0], ids, max_norm, name=name)\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py\", line 57, in _gather_and_clip\n    embs = array_ops.gather(params, ids, name=name)\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 2409, in gather\n    validate_indices=validate_indices, name=name)\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1219, in gather\n    validate_indices=validate_indices, name=name)\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/tycho/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): indices[113] = 10000 is not in [0, 10000)\n\t [[Node: embedding_lookup_8 = Gather[Tindices=DT_INT32, Tparams=DT_FLOAT, _class=[\"loc:@Variable_1\"], validate_indices=true, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](Variable_1/read, strided_slice_8)]]\n"
     ]
    }
   ],
   "source": [
    "doc2vec.fit(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
